{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BlueBikesRFPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for Bluebikes demand prediction using Random Forest.\n",
    "    Optimized for large datasets (6M+ rows) with proper time-based validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_path, sample_size=None, random_state=42, \n",
    "                 needs_preprocessing=True):\n",
    "        \"\"\"\n",
    "        Initialize pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pickle_path : str\n",
    "            Path to data pickle file (raw or preprocessed)\n",
    "        sample_size : int or float, optional\n",
    "            If int: number of rows to sample\n",
    "            If float: fraction of data to sample (e.g., 0.1 for 10%)\n",
    "            If None: use all data\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        needs_preprocessing : bool\n",
    "            If True, expects raw trip data and will aggregate/engineer features\n",
    "            If False, expects already preprocessed data\n",
    "        \"\"\"\n",
    "        self.pickle_path = pickle_path\n",
    "        self.sample_size = sample_size\n",
    "        self.random_state = random_state\n",
    "        self.needs_preprocessing = needs_preprocessing\n",
    "        self.pickup_model = None\n",
    "        self.dropoff_model = None\n",
    "        self.feature_names = None\n",
    "        self.results = {}\n",
    "        self.raw_df = None\n",
    "        self.df = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and optionally sample data\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        with open(self.pickle_path, 'rb') as f:\n",
    "            self.raw_df = pickle.load(f)\n",
    "        \n",
    "        print(f\"Original data shape: {self.raw_df.shape}\")\n",
    "        print(f\"Columns: {list(self.raw_df.columns)}\")\n",
    "        \n",
    "        # Sample data if specified (sample BEFORE preprocessing for speed)\n",
    "        if self.sample_size is not None:\n",
    "            if isinstance(self.sample_size, float):\n",
    "                n_samples = int(len(self.raw_df) * self.sample_size)\n",
    "            else:\n",
    "                n_samples = self.sample_size\n",
    "            \n",
    "            print(f\"Sampling {n_samples:,} rows for faster training...\")\n",
    "            self.raw_df = self.raw_df.sample(n=n_samples, random_state=self.random_state)\n",
    "            print(f\"Sampled data shape: {self.raw_df.shape}\")\n",
    "        \n",
    "        # Preprocess if needed\n",
    "        if self.needs_preprocessing:\n",
    "            self.df = self.preprocess_data()\n",
    "        else:\n",
    "            self.df = self.raw_df\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Transform raw trip data into aggregated station-level demand data\n",
    "        with engineered features\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PREPROCESSING RAW TRIP DATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df = self.raw_df.copy()\n",
    "        \n",
    "        # Ensure datetime columns\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['start_time']):\n",
    "            print(\"Converting start_time to datetime...\")\n",
    "            df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['stop_time']):\n",
    "            print(\"Converting stop_time to datetime...\")\n",
    "            df['stop_time'] = pd.to_datetime(df['stop_time'])\n",
    "        \n",
    "        # Step 1: Aggregate to hourly station-level pickups\n",
    "        print(\"\\nAggregating pickups by station and hour...\")\n",
    "        pickups = df.groupby([\n",
    "            pd.Grouper(key='start_time', freq='1H'),\n",
    "            'start_station_id'\n",
    "        ]).agg({\n",
    "            'ride_id': 'count',\n",
    "            'start_station_latitude': 'first',\n",
    "            'start_station_longitude': 'first',\n",
    "            'user_type': lambda x: (x == 'member').sum() / len(x) if len(x) > 0 else 0,\n",
    "        }).reset_index()\n",
    "        \n",
    "        pickups.columns = ['time', 'station_id', 'pickups', 'latitude', \n",
    "                          'longitude', 'member_ratio']\n",
    "        \n",
    "        # Step 2: Aggregate dropoffs by station and hour\n",
    "        print(\"Aggregating dropoffs by station and hour...\")\n",
    "        dropoffs = df.groupby([\n",
    "            pd.Grouper(key='stop_time', freq='1H'),\n",
    "            'end_station_id'\n",
    "        ]).agg({\n",
    "            'ride_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        dropoffs.columns = ['time', 'station_id', 'dropoffs']\n",
    "        \n",
    "        # Step 3: Merge pickups and dropoffs\n",
    "        print(\"Merging pickups and dropoffs...\")\n",
    "        station_demand = pd.merge(\n",
    "            pickups, \n",
    "            dropoffs, \n",
    "            on=['time', 'station_id'], \n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Calculate net flow\n",
    "        station_demand['net_flow'] = (station_demand['dropoffs'] - \n",
    "                                      station_demand['pickups'])\n",
    "        \n",
    "        # Step 4: Engineer temporal features\n",
    "        print(\"Engineering temporal features...\")\n",
    "        station_demand['hour'] = station_demand['time'].dt.hour\n",
    "        station_demand['day_of_week'] = station_demand['time'].dt.dayofweek\n",
    "        station_demand['month'] = station_demand['time'].dt.month\n",
    "        station_demand['day_of_year'] = station_demand['time'].dt.dayofyear\n",
    "        station_demand['week_of_year'] = station_demand['time'].dt.isocalendar().week\n",
    "        \n",
    "        # Binary features\n",
    "        station_demand['is_weekend'] = (station_demand['day_of_week'] >= 5).astype(int)\n",
    "        station_demand['is_rush_hour_morning'] = station_demand['hour'].isin([7, 8, 9]).astype(int)\n",
    "        station_demand['is_rush_hour_evening'] = station_demand['hour'].isin([17, 18, 19]).astype(int)\n",
    "        station_demand['is_business_hours'] = station_demand['hour'].between(9, 17).astype(int)\n",
    "        station_demand['is_night'] = ((station_demand['hour'] >= 22) | \n",
    "                                      (station_demand['hour'] <= 5)).astype(int)\n",
    "        \n",
    "        # Cyclical encoding for hour and day_of_week\n",
    "        station_demand['hour_sin'] = np.sin(2 * np.pi * station_demand['hour'] / 24)\n",
    "        station_demand['hour_cos'] = np.cos(2 * np.pi * station_demand['hour'] / 24)\n",
    "        station_demand['dow_sin'] = np.sin(2 * np.pi * station_demand['day_of_week'] / 7)\n",
    "        station_demand['dow_cos'] = np.cos(2 * np.pi * station_demand['day_of_week'] / 7)\n",
    "        \n",
    "        # Step 5: Engineer lag features (CRITICAL for time series)\n",
    "        print(\"Engineering lag features (this may take a moment)...\")\n",
    "        station_demand = station_demand.sort_values(['station_id', 'time'])\n",
    "        \n",
    "        # Create lag features for each station\n",
    "        for lag in [1, 24, 168]:  # 1 hour, 1 day, 1 week\n",
    "            station_demand[f'pickups_lag_{lag}h'] = station_demand.groupby('station_id')['pickups'].shift(lag)\n",
    "            station_demand[f'dropoffs_lag_{lag}h'] = station_demand.groupby('station_id')['dropoffs'].shift(lag)\n",
    "        \n",
    "        # Rolling averages\n",
    "        for window in [24, 168]:  # 1 day, 1 week\n",
    "            station_demand[f'pickups_rolling_{window}h'] = (\n",
    "                station_demand.groupby('station_id')['pickups']\n",
    "                .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "            station_demand[f'dropoffs_rolling_{window}h'] = (\n",
    "                station_demand.groupby('station_id')['dropoffs']\n",
    "                .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "        \n",
    "        # Step 6: Station-level features\n",
    "        print(\"Engineering station-level features...\")\n",
    "        station_stats = station_demand.groupby('station_id').agg({\n",
    "            'pickups': ['mean', 'std', 'max'],\n",
    "            'dropoffs': ['mean', 'std', 'max'],\n",
    "        })\n",
    "        station_stats.columns = ['_'.join(col) for col in station_stats.columns]\n",
    "        station_stats = station_stats.reset_index()\n",
    "        \n",
    "        station_demand = station_demand.merge(station_stats, on='station_id', how='left')\n",
    "        \n",
    "        # Step 7: Filter out very low activity periods\n",
    "        print(\"Filtering low-activity periods...\")\n",
    "        initial_rows_filter = len(station_demand)\n",
    "        \n",
    "        # Keep only stations with at least some minimum activity\n",
    "        station_total_activity = station_demand.groupby('station_id')[['pickups', 'dropoffs']].sum()\n",
    "        active_stations = station_total_activity[\n",
    "            (station_total_activity['pickups'] > 10) | \n",
    "            (station_total_activity['dropoffs'] > 10)\n",
    "        ].index\n",
    "        \n",
    "        station_demand = station_demand[station_demand['station_id'].isin(active_stations)]\n",
    "        filtered_rows = initial_rows_filter - len(station_demand)\n",
    "        \n",
    "        print(f\"  Filtered out {filtered_rows:,} rows from inactive stations\")\n",
    "        \n",
    "        # Drop rows with NaN (from lag features at the beginning)\n",
    "        initial_rows = len(station_demand)\n",
    "        station_demand = station_demand.dropna()\n",
    "        dropped_rows = initial_rows - len(station_demand)\n",
    "        \n",
    "        print(f\"\\nPreprocessing complete!\")\n",
    "        print(f\"  Original trips: {len(df):,}\")\n",
    "        print(f\"  Aggregated to: {len(station_demand):,} station-hour records\")\n",
    "        print(f\"  Active stations: {len(active_stations)}\")\n",
    "        print(f\"  Dropped {dropped_rows:,} rows due to lag features\")\n",
    "        print(f\"  Final features: {len(station_demand.columns)} columns\")\n",
    "        print(f\"  Date range: {station_demand['time'].min()} to {station_demand['time'].max()}\")\n",
    "        \n",
    "        return station_demand\n",
    "    \n",
    "    def prepare_features(self, target_col='pickups'):\n",
    "        \"\"\"\n",
    "        Prepare features and target variable\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str\n",
    "            Target variable name ('pickups', 'dropoffs', or 'net_flow')\n",
    "        \"\"\"\n",
    "        print(f\"\\nPreparing features for {target_col} prediction...\")\n",
    "        \n",
    "        # Check if target exists\n",
    "        if target_col not in self.df.columns:\n",
    "            raise ValueError(f\"Target column '{target_col}' not found in data. \"\n",
    "                           f\"Available columns: {list(self.df.columns)}\")\n",
    "        \n",
    "        # Identify feature columns (exclude targets, identifiers, and time)\n",
    "        exclude_cols = ['pickups', 'dropoffs', 'net_flow', 'ride_id', \n",
    "                       'start_time', 'stop_time', 'start_station_name', \n",
    "                       'end_station_name', 'time', 'station_id']\n",
    "        \n",
    "        self.feature_names = [col for col in self.df.columns \n",
    "                             if col not in exclude_cols and \n",
    "                             not self.df[col].isna().all()]\n",
    "        \n",
    "        X = self.df[self.feature_names]\n",
    "        y = self.df[target_col]\n",
    "        \n",
    "        print(f\"Features: {len(self.feature_names)}\")\n",
    "        print(f\"Target: {target_col}\")\n",
    "        print(f\"Target range: {y.min():.1f} to {y.max():.1f}\")\n",
    "        print(f\"Target mean: {y.mean():.2f}\")\n",
    "        print(f\"\\nFeature categories:\")\n",
    "        print(f\"  - Temporal: hour, day_of_week, month, etc.\")\n",
    "        print(f\"  - Lag features: pickups_lag_*, dropoffs_lag_*\")\n",
    "        print(f\"  - Rolling features: *_rolling_*\")\n",
    "        print(f\"  - Station stats: *_mean, *_std, *_max\")\n",
    "        print(f\"  - Location: latitude, longitude\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def split_data(self, X, y, test_size=0.2, time_based=True):\n",
    "        \"\"\"\n",
    "        Split data into train/test sets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float\n",
    "            Proportion of data for testing\n",
    "        time_based : bool\n",
    "            If True, use time-based split (HIGHLY recommended for time series)\n",
    "            If False, use random split (NOT recommended)\n",
    "        \"\"\"\n",
    "        print(f\"\\nSplitting data (test_size={test_size})...\")\n",
    "        \n",
    "        if time_based and 'time' in self.df.columns:\n",
    "            # Sort by time first\n",
    "            sorted_df = self.df.sort_values('time').reset_index(drop=True)\n",
    "            \n",
    "            # Calculate split point\n",
    "            split_idx = int(len(sorted_df) * (1 - test_size))\n",
    "            \n",
    "            # Get indices for train/test\n",
    "            train_indices = sorted_df.index[:split_idx]\n",
    "            test_indices = sorted_df.index[split_idx:]\n",
    "            \n",
    "            # Split using original indices\n",
    "            X_train = X.iloc[train_indices]\n",
    "            X_test = X.iloc[test_indices]\n",
    "            y_train = y.iloc[train_indices]\n",
    "            y_test = y.iloc[test_indices]\n",
    "            \n",
    "            # Get time ranges for reporting\n",
    "            train_times = sorted_df.loc[train_indices, 'time']\n",
    "            test_times = sorted_df.loc[test_indices, 'time']\n",
    "            \n",
    "            print(f\"‚úì Using time-based split (RECOMMENDED)\")\n",
    "            print(f\"  Train period: {train_times.min()} to {train_times.max()}\")\n",
    "            print(f\"  Test period:  {test_times.min()} to {test_times.max()}\")\n",
    "        else:\n",
    "            # Random split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=self.random_state\n",
    "            )\n",
    "            print(\"‚ö†Ô∏è  Using random split (NOT recommended for time series)\")\n",
    "        \n",
    "        print(f\"Train set: {X_train.shape[0]:,} rows\")\n",
    "        print(f\"Test set: {X_test.shape[0]:,} rows\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train_model(self, X_train, y_train, target_name='pickups', \n",
    "                   n_estimators=100, max_depth=20, min_samples_split=20,\n",
    "                   min_samples_leaf=10, max_features='sqrt', n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Train Random Forest model with optimized parameters for large datasets\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Random Forest for {target_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=self.random_state,\n",
    "            verbose=1,\n",
    "            warm_start=False,\n",
    "            oob_score=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters:\")\n",
    "        print(f\"  - Trees: {n_estimators}\")\n",
    "        print(f\"  - Max depth: {max_depth}\")\n",
    "        print(f\"  - Min samples split: {min_samples_split}\")\n",
    "        print(f\"  - Min samples leaf: {min_samples_leaf}\")\n",
    "        print(f\"  - Max features: {max_features}\")\n",
    "        print(f\"  - Parallel jobs: {n_jobs}\")\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"\\n‚úì Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"  OOB Score (R¬≤): {model.oob_score_:.4f}\")\n",
    "        \n",
    "        return model, training_time\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, target_name='pickups'):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(f\"\\nEvaluating {target_name} model...\")\n",
    "        \n",
    "        # Predictions\n",
    "        start_time = datetime.now()\n",
    "        y_pred = model.predict(X_test)\n",
    "        prediction_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1))) * 100\n",
    "        \n",
    "        results = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'prediction_time': prediction_time,\n",
    "            'predictions': y_pred,\n",
    "            'actuals': y_test\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{target_name.upper()} Model Performance:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"  MAE:  {mae:.2f}\")\n",
    "        print(f\"  RMSE: {rmse:.2f}\")\n",
    "        print(f\"  R¬≤:   {r2:.4f}\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "        print(f\"  Prediction time: {prediction_time:.2f}s for {len(X_test):,} samples\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_feature_importance(self, model, target_name='pickups', top_n=20):\n",
    "        \"\"\"Plot top N most important features\"\"\"\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')\n",
    "        plt.title(f'Top {top_n} Feature Importances - {target_name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_importance_{target_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\n‚úì Feature importance plot saved as 'feature_importance_{target_name}.png'\")\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def plot_predictions(self, results, target_name='pickups', n_samples=1000):\n",
    "        \"\"\"Plot actual vs predicted values\"\"\"\n",
    "        if len(results['actuals']) > n_samples:\n",
    "            indices = np.random.choice(len(results['actuals']), n_samples, replace=False)\n",
    "            actuals_plot = results['actuals'].iloc[indices]\n",
    "            predictions_plot = results['predictions'][indices]\n",
    "        else:\n",
    "            actuals_plot = results['actuals']\n",
    "            predictions_plot = results['predictions']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        axes[0].scatter(actuals_plot, predictions_plot, alpha=0.5, s=10)\n",
    "        axes[0].plot([actuals_plot.min(), actuals_plot.max()], \n",
    "                     [actuals_plot.min(), actuals_plot.max()], \n",
    "                     'r--', lw=2, label='Perfect prediction')\n",
    "        axes[0].set_xlabel('Actual')\n",
    "        axes[0].set_ylabel('Predicted')\n",
    "        axes[0].set_title(f'Actual vs Predicted - {target_name}')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = actuals_plot - predictions_plot\n",
    "        axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "        axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title(f'Residuals Distribution - {target_name}')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'predictions_{target_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Prediction plots saved as 'predictions_{target_name}.png'\")\n",
    "    \n",
    "    def save_model(self, model, target_name='pickups'):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        filename = f'rf_model_{target_name}.joblib'\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"\\n‚úì Model saved as '{filename}'\")\n",
    "    \n",
    "    def run_full_pipeline(self, train_pickups=True, train_dropoffs=True):\n",
    "        \"\"\"\n",
    "        Run complete pipeline for both pickups and dropoffs prediction\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BLUEBIKES DEMAND PREDICTION PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Train pickups model\n",
    "        if train_pickups and 'pickups' in self.df.columns:\n",
    "            X, y = self.prepare_features(target_col='pickups')\n",
    "            X_train, X_test, y_train, y_test = self.split_data(X, y, time_based=True)\n",
    "            \n",
    "            self.pickup_model, train_time = self.train_model(\n",
    "                X_train, y_train, target_name='pickups'\n",
    "            )\n",
    "            \n",
    "            self.results['pickups'] = self.evaluate_model(\n",
    "                self.pickup_model, X_test, y_test, target_name='pickups'\n",
    "            )\n",
    "            \n",
    "            self.plot_feature_importance(self.pickup_model, target_name='pickups')\n",
    "            self.plot_predictions(self.results['pickups'], target_name='pickups')\n",
    "            self.save_model(self.pickup_model, target_name='pickups')\n",
    "        \n",
    "        # Train dropoffs model\n",
    "        if train_dropoffs and 'dropoffs' in self.df.columns:\n",
    "            X, y = self.prepare_features(target_col='dropoffs')\n",
    "            X_train, X_test, y_train, y_test = self.split_data(X, y, time_based=True)\n",
    "            \n",
    "            self.dropoff_model, train_time = self.train_model(\n",
    "                X_train, y_train, target_name='dropoffs'\n",
    "            )\n",
    "            \n",
    "            self.results['dropoffs'] = self.evaluate_model(\n",
    "                self.dropoff_model, X_test, y_test, target_name='dropoffs'\n",
    "            )\n",
    "            \n",
    "            self.plot_feature_importance(self.dropoff_model, target_name='dropoffs')\n",
    "            self.plot_predictions(self.results['dropoffs'], target_name='dropoffs')\n",
    "            self.save_model(self.dropoff_model, target_name='dropoffs')\n",
    "        \n",
    "        # Calculate net flow if both models trained\n",
    "        if train_pickups and train_dropoffs and self.pickup_model and self.dropoff_model:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"NET FLOW ANALYSIS (Dropoffs - Pickups)\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            net_flow_pred = (self.results['dropoffs']['predictions'] - \n",
    "                           self.results['pickups']['predictions'])\n",
    "            net_flow_actual = (self.results['dropoffs']['actuals'].values - \n",
    "                             self.results['pickups']['actuals'].values)\n",
    "            \n",
    "            mae_netflow = mean_absolute_error(net_flow_actual, net_flow_pred)\n",
    "            rmse_netflow = np.sqrt(mean_squared_error(net_flow_actual, net_flow_pred))\n",
    "            r2_netflow = r2_score(net_flow_actual, net_flow_pred)\n",
    "            \n",
    "            print(f\"\\nNet Flow Prediction Performance:\")\n",
    "            print(f\"  MAE:  {mae_netflow:.2f}\")\n",
    "            print(f\"  RMSE: {rmse_netflow:.2f}\")\n",
    "            print(f\"  R¬≤:   {r2_netflow:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f3fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range input for training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BlueBikesRFPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for Bluebikes demand prediction using Random Forest.\n",
    "    Optimized for large datasets (6M+ rows) with proper time-based validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_path, sample_size=None, random_state=42, \n",
    "                 needs_preprocessing=True, train_start_date=None, train_end_date=None):\n",
    "        \"\"\"\n",
    "        Initialize pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pickle_path : str\n",
    "            Path to data pickle file (raw or preprocessed)\n",
    "        sample_size : int or float, optional\n",
    "            If int: number of rows to sample\n",
    "            If float: fraction of data to sample (e.g., 0.1 for 10%)\n",
    "            If None: use all data\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        needs_preprocessing : bool\n",
    "            If True, expects raw trip data and will aggregate/engineer features\n",
    "            If False, expects already preprocessed data\n",
    "        train_start_date : str, optional\n",
    "            Start date for training data (e.g., '2025-01-01')\n",
    "            If None, uses earliest date in data\n",
    "        train_end_date : str, optional\n",
    "            End date for training data (e.g., '2025-06-30')\n",
    "            If None, uses latest date in data\n",
    "        \"\"\"\n",
    "        self.pickle_path = pickle_path\n",
    "        self.sample_size = sample_size\n",
    "        self.random_state = random_state\n",
    "        self.needs_preprocessing = needs_preprocessing\n",
    "        self.train_start_date = pd.to_datetime(train_start_date, utc=True) if train_start_date else None\n",
    "        self.train_end_date = pd.to_datetime(train_end_date, utc=True) if train_end_date else None\n",
    "        self.pickup_model = None\n",
    "        self.dropoff_model = None\n",
    "        self.feature_names = None\n",
    "        self.results = {}\n",
    "        self.raw_df = None\n",
    "        self.df = None\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"BLUEBIKES RANDOM FOREST PIPELINE\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Data: {pickle_path}\")\n",
    "        print(f\"Sample size: {sample_size if sample_size else 'ALL (no sampling)'}\")\n",
    "        if train_start_date:\n",
    "            print(f\"Training start: {train_start_date}\")\n",
    "        else:\n",
    "            print(f\"Training start: Earliest date in data\")\n",
    "        if train_end_date:\n",
    "            print(f\"Training end: {train_end_date}\")\n",
    "        else:\n",
    "            print(f\"Training end: Latest date in data\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and optionally sample data\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        with open(self.pickle_path, 'rb') as f:\n",
    "            self.raw_df = pickle.load(f)\n",
    "        \n",
    "        print(f\"Original data shape: {self.raw_df.shape}\")\n",
    "        print(f\"Columns: {list(self.raw_df.columns)}\")\n",
    "        \n",
    "        # Ensure datetime columns are timezone-aware\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.raw_df['start_time']):\n",
    "            self.raw_df['start_time'] = pd.to_datetime(self.raw_df['start_time'], utc=True)\n",
    "        elif self.raw_df['start_time'].dt.tz is None:\n",
    "            self.raw_df['start_time'] = self.raw_df['start_time'].dt.tz_localize('UTC')\n",
    "            \n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.raw_df['stop_time']):\n",
    "            self.raw_df['stop_time'] = pd.to_datetime(self.raw_df['stop_time'], utc=True)\n",
    "        elif self.raw_df['stop_time'].dt.tz is None:\n",
    "            self.raw_df['stop_time'] = self.raw_df['stop_time'].dt.tz_localize('UTC')\n",
    "        \n",
    "        print(f\"Full date range: {self.raw_df['start_time'].min()} to {self.raw_df['start_time'].max()}\")\n",
    "        \n",
    "        # Filter by training date range if specified\n",
    "        if self.train_start_date is not None or self.train_end_date is not None:\n",
    "            print(f\"\\nüéØ FILTERING TO TRAINING PERIOD:\")\n",
    "            \n",
    "            if self.train_start_date is not None:\n",
    "                before_filter = len(self.raw_df)\n",
    "                self.raw_df = self.raw_df[self.raw_df['start_time'] >= self.train_start_date]\n",
    "                print(f\"  After start date filter: {len(self.raw_df):,} rows (removed {before_filter - len(self.raw_df):,})\")\n",
    "            \n",
    "            if self.train_end_date is not None:\n",
    "                before_filter = len(self.raw_df)\n",
    "                self.raw_df = self.raw_df[self.raw_df['start_time'] <= self.train_end_date]\n",
    "                print(f\"  After end date filter: {len(self.raw_df):,} rows (removed {before_filter - len(self.raw_df):,})\")\n",
    "            \n",
    "            print(f\"‚úì Training date range: {self.raw_df['start_time'].min()} to {self.raw_df['start_time'].max()}\")\n",
    "        \n",
    "        # Sample data if specified (sample AFTER date filtering)\n",
    "        if self.sample_size is not None:\n",
    "            if isinstance(self.sample_size, float):\n",
    "                n_samples = int(len(self.raw_df) * self.sample_size)\n",
    "            else:\n",
    "                n_samples = self.sample_size\n",
    "            \n",
    "            print(f\"\\nSampling {n_samples:,} rows for faster training...\")\n",
    "            self.raw_df = self.raw_df.sample(n=n_samples, random_state=self.random_state)\n",
    "            print(f\"Sampled data shape: {self.raw_df.shape}\")\n",
    "        \n",
    "        # Preprocess if needed\n",
    "        if self.needs_preprocessing:\n",
    "            self.df = self.preprocess_data()\n",
    "        else:\n",
    "            self.df = self.raw_df\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Transform raw trip data into aggregated station-level demand data\n",
    "        with engineered features\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PREPROCESSING RAW TRIP DATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df = self.raw_df.copy()\n",
    "        \n",
    "        # Ensure datetime columns\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['start_time']):\n",
    "            print(\"Converting start_time to datetime...\")\n",
    "            df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['stop_time']):\n",
    "            print(\"Converting stop_time to datetime...\")\n",
    "            df['stop_time'] = pd.to_datetime(df['stop_time'])\n",
    "        \n",
    "        # Step 1: Aggregate to hourly station-level pickups\n",
    "        print(\"\\nAggregating pickups by station and hour...\")\n",
    "        pickups = df.groupby([\n",
    "            pd.Grouper(key='start_time', freq='1H'),\n",
    "            'start_station_id'\n",
    "        ]).agg({\n",
    "            'ride_id': 'count',\n",
    "            'start_station_latitude': 'first',\n",
    "            'start_station_longitude': 'first',\n",
    "            'user_type': lambda x: (x == 'member').sum() / len(x) if len(x) > 0 else 0,\n",
    "        }).reset_index()\n",
    "        \n",
    "        pickups.columns = ['time', 'station_id', 'pickups', 'latitude', \n",
    "                          'longitude', 'member_ratio']\n",
    "        \n",
    "        # Step 2: Aggregate dropoffs by station and hour\n",
    "        print(\"Aggregating dropoffs by station and hour...\")\n",
    "        dropoffs = df.groupby([\n",
    "            pd.Grouper(key='stop_time', freq='1H'),\n",
    "            'end_station_id'\n",
    "        ]).agg({\n",
    "            'ride_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        dropoffs.columns = ['time', 'station_id', 'dropoffs']\n",
    "        \n",
    "        # Step 3: Merge pickups and dropoffs\n",
    "        print(\"Merging pickups and dropoffs...\")\n",
    "        station_demand = pd.merge(\n",
    "            pickups, \n",
    "            dropoffs, \n",
    "            on=['time', 'station_id'], \n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Calculate net flow\n",
    "        station_demand['net_flow'] = (station_demand['dropoffs'] - \n",
    "                                      station_demand['pickups'])\n",
    "        \n",
    "        # Step 4: Engineer temporal features\n",
    "        print(\"Engineering temporal features...\")\n",
    "        station_demand['hour'] = station_demand['time'].dt.hour\n",
    "        station_demand['day_of_week'] = station_demand['time'].dt.dayofweek\n",
    "        station_demand['month'] = station_demand['time'].dt.month\n",
    "        station_demand['day_of_year'] = station_demand['time'].dt.dayofyear\n",
    "        station_demand['week_of_year'] = station_demand['time'].dt.isocalendar().week\n",
    "        \n",
    "        # Binary features\n",
    "        station_demand['is_weekend'] = (station_demand['day_of_week'] >= 5).astype(int)\n",
    "        station_demand['is_rush_hour_morning'] = station_demand['hour'].isin([7, 8, 9]).astype(int)\n",
    "        station_demand['is_rush_hour_evening'] = station_demand['hour'].isin([17, 18, 19]).astype(int)\n",
    "        station_demand['is_business_hours'] = station_demand['hour'].between(9, 17).astype(int)\n",
    "        station_demand['is_night'] = ((station_demand['hour'] >= 22) | \n",
    "                                      (station_demand['hour'] <= 5)).astype(int)\n",
    "        \n",
    "        # Cyclical encoding for hour and day_of_week\n",
    "        station_demand['hour_sin'] = np.sin(2 * np.pi * station_demand['hour'] / 24)\n",
    "        station_demand['hour_cos'] = np.cos(2 * np.pi * station_demand['hour'] / 24)\n",
    "        station_demand['dow_sin'] = np.sin(2 * np.pi * station_demand['day_of_week'] / 7)\n",
    "        station_demand['dow_cos'] = np.cos(2 * np.pi * station_demand['day_of_week'] / 7)\n",
    "        \n",
    "        # Step 5: Engineer lag features (CRITICAL for time series)\n",
    "        print(\"Engineering lag features (this may take a moment)...\")\n",
    "        station_demand = station_demand.sort_values(['station_id', 'time'])\n",
    "        \n",
    "        # Create lag features for each station\n",
    "        for lag in [1, 24, 168]:  # 1 hour, 1 day, 1 week\n",
    "            station_demand[f'pickups_lag_{lag}h'] = station_demand.groupby('station_id')['pickups'].shift(lag)\n",
    "            station_demand[f'dropoffs_lag_{lag}h'] = station_demand.groupby('station_id')['dropoffs'].shift(lag)\n",
    "        \n",
    "        # Rolling averages\n",
    "        for window in [24, 168]:  # 1 day, 1 week\n",
    "            station_demand[f'pickups_rolling_{window}h'] = (\n",
    "                station_demand.groupby('station_id')['pickups']\n",
    "                .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "            station_demand[f'dropoffs_rolling_{window}h'] = (\n",
    "                station_demand.groupby('station_id')['dropoffs']\n",
    "                .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "        \n",
    "        # Step 6: Station-level features\n",
    "        print(\"Engineering station-level features...\")\n",
    "        station_stats = station_demand.groupby('station_id').agg({\n",
    "            'pickups': ['mean', 'std', 'max'],\n",
    "            'dropoffs': ['mean', 'std', 'max'],\n",
    "        })\n",
    "        station_stats.columns = ['_'.join(col) for col in station_stats.columns]\n",
    "        station_stats = station_stats.reset_index()\n",
    "        \n",
    "        station_demand = station_demand.merge(station_stats, on='station_id', how='left')\n",
    "        \n",
    "        # Step 7: Filter out very low activity periods\n",
    "        print(\"Filtering low-activity periods...\")\n",
    "        initial_rows_filter = len(station_demand)\n",
    "        \n",
    "        # Keep only stations with at least some minimum activity\n",
    "        station_total_activity = station_demand.groupby('station_id')[['pickups', 'dropoffs']].sum()\n",
    "        active_stations = station_total_activity[\n",
    "            (station_total_activity['pickups'] > 10) | \n",
    "            (station_total_activity['dropoffs'] > 10)\n",
    "        ].index\n",
    "        \n",
    "        station_demand = station_demand[station_demand['station_id'].isin(active_stations)]\n",
    "        filtered_rows = initial_rows_filter - len(station_demand)\n",
    "        \n",
    "        print(f\"  Filtered out {filtered_rows:,} rows from inactive stations\")\n",
    "        \n",
    "        # Drop rows with NaN (from lag features at the beginning)\n",
    "        initial_rows = len(station_demand)\n",
    "        station_demand = station_demand.dropna()\n",
    "        dropped_rows = initial_rows - len(station_demand)\n",
    "        \n",
    "        print(f\"\\nPreprocessing complete!\")\n",
    "        print(f\"  Original trips: {len(df):,}\")\n",
    "        print(f\"  Aggregated to: {len(station_demand):,} station-hour records\")\n",
    "        print(f\"  Active stations: {len(active_stations)}\")\n",
    "        print(f\"  Dropped {dropped_rows:,} rows due to lag features\")\n",
    "        print(f\"  Final features: {len(station_demand.columns)} columns\")\n",
    "        print(f\"  Date range: {station_demand['time'].min()} to {station_demand['time'].max()}\")\n",
    "        \n",
    "        return station_demand\n",
    "    \n",
    "    def prepare_features(self, target_col='pickups'):\n",
    "        \"\"\"\n",
    "        Prepare features and target variable\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str\n",
    "            Target variable name ('pickups', 'dropoffs', or 'net_flow')\n",
    "        \"\"\"\n",
    "        print(f\"\\nPreparing features for {target_col} prediction...\")\n",
    "        \n",
    "        # Check if target exists\n",
    "        if target_col not in self.df.columns:\n",
    "            raise ValueError(f\"Target column '{target_col}' not found in data. \"\n",
    "                           f\"Available columns: {list(self.df.columns)}\")\n",
    "        \n",
    "        # Identify feature columns (exclude targets, identifiers, and time)\n",
    "        exclude_cols = ['pickups', 'dropoffs', 'net_flow', 'ride_id', \n",
    "                       'start_time', 'stop_time', 'start_station_name', \n",
    "                       'end_station_name', 'time', 'station_id']\n",
    "        \n",
    "        self.feature_names = [col for col in self.df.columns \n",
    "                             if col not in exclude_cols and \n",
    "                             not self.df[col].isna().all()]\n",
    "        \n",
    "        X = self.df[self.feature_names]\n",
    "        y = self.df[target_col]\n",
    "        \n",
    "        print(f\"Features: {len(self.feature_names)}\")\n",
    "        print(f\"Target: {target_col}\")\n",
    "        print(f\"Target range: {y.min():.1f} to {y.max():.1f}\")\n",
    "        print(f\"Target mean: {y.mean():.2f}\")\n",
    "        print(f\"\\nFeature categories:\")\n",
    "        print(f\"  - Temporal: hour, day_of_week, month, etc.\")\n",
    "        print(f\"  - Lag features: pickups_lag_*, dropoffs_lag_*\")\n",
    "        print(f\"  - Rolling features: *_rolling_*\")\n",
    "        print(f\"  - Station stats: *_mean, *_std, *_max\")\n",
    "        print(f\"  - Location: latitude, longitude\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def split_data(self, X, y, test_size=0.2, time_based=True):\n",
    "        \"\"\"\n",
    "        Split data into train/test sets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float\n",
    "            Proportion of data for testing\n",
    "        time_based : bool\n",
    "            If True, use time-based split (HIGHLY recommended for time series)\n",
    "            If False, use random split (NOT recommended)\n",
    "        \"\"\"\n",
    "        print(f\"\\nSplitting data (test_size={test_size})...\")\n",
    "        \n",
    "        if time_based and 'time' in self.df.columns:\n",
    "            # Sort by time first\n",
    "            sorted_df = self.df.sort_values('time').reset_index(drop=True)\n",
    "            \n",
    "            # Calculate split point\n",
    "            split_idx = int(len(sorted_df) * (1 - test_size))\n",
    "            \n",
    "            # Get indices for train/test\n",
    "            train_indices = sorted_df.index[:split_idx]\n",
    "            test_indices = sorted_df.index[split_idx:]\n",
    "            \n",
    "            # Split using original indices\n",
    "            X_train = X.iloc[train_indices]\n",
    "            X_test = X.iloc[test_indices]\n",
    "            y_train = y.iloc[train_indices]\n",
    "            y_test = y.iloc[test_indices]\n",
    "            \n",
    "            # Get time ranges for reporting\n",
    "            train_times = sorted_df.loc[train_indices, 'time']\n",
    "            test_times = sorted_df.loc[test_indices, 'time']\n",
    "            \n",
    "            print(f\"‚úì Using time-based split (RECOMMENDED)\")\n",
    "            print(f\"  Train period: {train_times.min()} to {train_times.max()}\")\n",
    "            print(f\"  Test period:  {test_times.min()} to {test_times.max()}\")\n",
    "        else:\n",
    "            # Random split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=self.random_state\n",
    "            )\n",
    "            print(\"‚ö†Ô∏è  Using random split (NOT recommended for time series)\")\n",
    "        \n",
    "        print(f\"Train set: {X_train.shape[0]:,} rows\")\n",
    "        print(f\"Test set: {X_test.shape[0]:,} rows\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train_model(self, X_train, y_train, target_name='pickups', \n",
    "                   n_estimators=100, max_depth=20, min_samples_split=20,\n",
    "                   min_samples_leaf=10, max_features='sqrt', n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Train Random Forest model with optimized parameters for large datasets\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Random Forest for {target_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=self.random_state,\n",
    "            verbose=1,\n",
    "            warm_start=False,\n",
    "            oob_score=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel parameters:\")\n",
    "        print(f\"  - Trees: {n_estimators}\")\n",
    "        print(f\"  - Max depth: {max_depth}\")\n",
    "        print(f\"  - Min samples split: {min_samples_split}\")\n",
    "        print(f\"  - Min samples leaf: {min_samples_leaf}\")\n",
    "        print(f\"  - Max features: {max_features}\")\n",
    "        print(f\"  - Parallel jobs: {n_jobs}\")\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"\\n‚úì Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"  OOB Score (R¬≤): {model.oob_score_:.4f}\")\n",
    "        \n",
    "        return model, training_time\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, target_name='pickups'):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(f\"\\nEvaluating {target_name} model...\")\n",
    "        \n",
    "        # Predictions\n",
    "        start_time = datetime.now()\n",
    "        y_pred = model.predict(X_test)\n",
    "        prediction_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1))) * 100\n",
    "        \n",
    "        results = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'prediction_time': prediction_time,\n",
    "            'predictions': y_pred,\n",
    "            'actuals': y_test\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{target_name.upper()} Model Performance:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"  MAE:  {mae:.2f}\")\n",
    "        print(f\"  RMSE: {rmse:.2f}\")\n",
    "        print(f\"  R¬≤:   {r2:.4f}\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "        print(f\"  Prediction time: {prediction_time:.2f}s for {len(X_test):,} samples\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_feature_importance(self, model, target_name='pickups', top_n=20):\n",
    "        \"\"\"Plot top N most important features\"\"\"\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')\n",
    "        plt.title(f'Top {top_n} Feature Importances - {target_name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_importance_{target_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\n‚úì Feature importance plot saved as 'feature_importance_{target_name}.png'\")\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def plot_predictions(self, results, target_name='pickups', n_samples=1000):\n",
    "        \"\"\"Plot actual vs predicted values\"\"\"\n",
    "        if len(results['actuals']) > n_samples:\n",
    "            indices = np.random.choice(len(results['actuals']), n_samples, replace=False)\n",
    "            actuals_plot = results['actuals'].iloc[indices]\n",
    "            predictions_plot = results['predictions'][indices]\n",
    "        else:\n",
    "            actuals_plot = results['actuals']\n",
    "            predictions_plot = results['predictions']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        axes[0].scatter(actuals_plot, predictions_plot, alpha=0.5, s=10)\n",
    "        axes[0].plot([actuals_plot.min(), actuals_plot.max()], \n",
    "                     [actuals_plot.min(), actuals_plot.max()], \n",
    "                     'r--', lw=2, label='Perfect prediction')\n",
    "        axes[0].set_xlabel('Actual')\n",
    "        axes[0].set_ylabel('Predicted')\n",
    "        axes[0].set_title(f'Actual vs Predicted - {target_name}')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals\n",
    "        residuals = actuals_plot - predictions_plot\n",
    "        axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "        axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title(f'Residuals Distribution - {target_name}')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'predictions_{target_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Prediction plots saved as 'predictions_{target_name}.png'\")\n",
    "    \n",
    "    def save_model(self, model, target_name='pickups'):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        filename = f'rf_model_{target_name}.joblib'\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"\\n‚úì Model saved as '{filename}'\")\n",
    "    \n",
    "    def run_full_pipeline(self, train_pickups=True, train_dropoffs=True):\n",
    "        \"\"\"\n",
    "        Run complete pipeline for both pickups and dropoffs prediction\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BLUEBIKES DEMAND PREDICTION PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Train pickups model\n",
    "        if train_pickups and 'pickups' in self.df.columns:\n",
    "            X, y = self.prepare_features(target_col='pickups')\n",
    "            X_train, X_test, y_train, y_test = self.split_data(X, y, time_based=True)\n",
    "            \n",
    "            self.pickup_model, train_time = self.train_model(\n",
    "                X_train, y_train, target_name='pickups'\n",
    "            )\n",
    "            \n",
    "            self.results['pickups'] = self.evaluate_model(\n",
    "                self.pickup_model, X_test, y_test, target_name='pickups'\n",
    "            )\n",
    "            \n",
    "            self.plot_feature_importance(self.pickup_model, target_name='pickups')\n",
    "            self.plot_predictions(self.results['pickups'], target_name='pickups')\n",
    "            self.save_model(self.pickup_model, target_name='pickups')\n",
    "        \n",
    "        # Train dropoffs model\n",
    "        if train_dropoffs and 'dropoffs' in self.df.columns:\n",
    "            X, y = self.prepare_features(target_col='dropoffs')\n",
    "            X_train, X_test, y_train, y_test = self.split_data(X, y, time_based=True)\n",
    "            \n",
    "            self.dropoff_model, train_time = self.train_model(\n",
    "                X_train, y_train, target_name='dropoffs'\n",
    "            )\n",
    "            \n",
    "            self.results['dropoffs'] = self.evaluate_model(\n",
    "                self.dropoff_model, X_test, y_test, target_name='dropoffs'\n",
    "            )\n",
    "            \n",
    "            self.plot_feature_importance(self.dropoff_model, target_name='dropoffs')\n",
    "            self.plot_predictions(self.results['dropoffs'], target_name='dropoffs')\n",
    "            self.save_model(self.dropoff_model, target_name='dropoffs')\n",
    "        \n",
    "        # Calculate net flow if both models trained\n",
    "        if train_pickups and train_dropoffs and self.pickup_model and self.dropoff_model:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"NET FLOW ANALYSIS (Dropoffs - Pickups)\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            net_flow_pred = (self.results['dropoffs']['predictions'] - \n",
    "                           self.results['pickups']['predictions'])\n",
    "            net_flow_actual = (self.results['dropoffs']['actuals'].values - \n",
    "                             self.results['pickups']['actuals'].values)\n",
    "            \n",
    "            mae_netflow = mean_absolute_error(net_flow_actual, net_flow_pred)\n",
    "            rmse_netflow = np.sqrt(mean_squared_error(net_flow_actual, net_flow_pred))\n",
    "            r2_netflow = r2_score(net_flow_actual, net_flow_pred)\n",
    "            \n",
    "            print(f\"\\nNet Flow Prediction Performance:\")\n",
    "            print(f\"  MAE:  {mae_netflow:.2f}\")\n",
    "            print(f\"  RMSE: {rmse_netflow:.2f}\")\n",
    "            print(f\"  R¬≤:   {r2_netflow:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942261e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BLUEBIKES RANDOM FOREST PIPELINE\n",
      "======================================================================\n",
      "Data: ../data_pipeline/data/processed/bluebikes/after_duplicates.pkl\n",
      "Sample size: 0.8\n",
      "Training start: 2023-12-31\n",
      "Training end: 2025-06-30\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "BLUEBIKES DEMAND PREDICTION PIPELINE\n",
      "============================================================\n",
      "Loading data...\n",
      "Original data shape: (2084559, 13)\n",
      "Columns: ['ride_id', 'rideable_type', 'start_time', 'stop_time', 'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id', 'start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'user_type']\n",
      "Full date range: 2024-01-01 00:05:08+00:00 to 2025-10-31 23:56:01.017000+00:00\n",
      "\n",
      "üéØ FILTERING TO TRAINING PERIOD:\n",
      "  After start date filter: 1,381,928 rows (removed 702,631)\n",
      "  After end date filter: 831,372 rows (removed 550,556)\n",
      "‚úì Training date range: 2024-01-01 00:05:08+00:00 to 2025-06-29 23:56:08.608000+00:00\n",
      "\n",
      "Sampling 665,097 rows for faster training...\n",
      "Sampled data shape: (665097, 13)\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING RAW TRIP DATA\n",
      "============================================================\n",
      "\n",
      "Aggregating pickups by station and hour...\n",
      "Aggregating dropoffs by station and hour...\n",
      "Merging pickups and dropoffs...\n",
      "Engineering temporal features...\n",
      "Engineering lag features (this may take a moment)...\n",
      "Engineering station-level features...\n",
      "Filtering low-activity periods...\n",
      "  Filtered out 44 rows from inactive stations\n",
      "\n",
      "Preprocessing complete!\n",
      "  Original trips: 665,097\n",
      "  Aggregated to: 681,221 station-hour records\n",
      "  Active stations: 573\n",
      "  Dropped 88,798 rows due to lag features\n",
      "  Final features: 38 columns\n",
      "  Date range: 2024-01-13 19:00:00+00:00 to 2025-06-30 07:00:00+00:00\n",
      "\n",
      "Preparing features for pickups prediction...\n",
      "Features: 33\n",
      "Target: pickups\n",
      "Target range: 0.0 to 18.0\n",
      "Target mean: 0.89\n",
      "\n",
      "Feature categories:\n",
      "  - Temporal: hour, day_of_week, month, etc.\n",
      "  - Lag features: pickups_lag_*, dropoffs_lag_*\n",
      "  - Rolling features: *_rolling_*\n",
      "  - Station stats: *_mean, *_std, *_max\n",
      "  - Location: latitude, longitude\n",
      "\n",
      "Splitting data (test_size=0.2)...\n",
      "‚úì Using time-based split (RECOMMENDED)\n",
      "  Train period: 2024-01-13 19:00:00+00:00 to 2025-05-18 15:00:00+00:00\n",
      "  Test period:  2025-05-18 15:00:00+00:00 to 2025-06-30 07:00:00+00:00\n",
      "Train set: 544,976 rows\n",
      "Test set: 136,245 rows\n",
      "\n",
      "============================================================\n",
      "Training Random Forest for pickups\n",
      "============================================================\n",
      "\n",
      "Model parameters:\n",
      "  - Trees: 100\n",
      "  - Max depth: 20\n",
      "  - Min samples split: 20\n",
      "  - Min samples leaf: 10\n",
      "  - Max features: sqrt\n",
      "  - Parallel jobs: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   17.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Training completed in 23.61 seconds\n",
      "  OOB Score (R¬≤): 0.7517\n",
      "\n",
      "Evaluating pickups model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PICKUPS Model Performance:\n",
      "==================================================\n",
      "  MAE:  0.21\n",
      "  RMSE: 0.44\n",
      "  R¬≤:   0.7613\n",
      "  MAPE: 8.09%\n",
      "  Prediction time: 0.32s for 136,245 samples\n",
      "\n",
      "‚úì Feature importance plot saved as 'feature_importance_pickups.png'\n",
      "‚úì Prediction plots saved as 'predictions_pickups.png'\n",
      "\n",
      "‚úì Model saved as 'rf_model_pickups.joblib'\n",
      "\n",
      "Preparing features for dropoffs prediction...\n",
      "Features: 33\n",
      "Target: dropoffs\n",
      "Target range: 0.0 to 18.0\n",
      "Target mean: 0.89\n",
      "\n",
      "Feature categories:\n",
      "  - Temporal: hour, day_of_week, month, etc.\n",
      "  - Lag features: pickups_lag_*, dropoffs_lag_*\n",
      "  - Rolling features: *_rolling_*\n",
      "  - Station stats: *_mean, *_std, *_max\n",
      "  - Location: latitude, longitude\n",
      "\n",
      "Splitting data (test_size=0.2)...\n",
      "‚úì Using time-based split (RECOMMENDED)\n",
      "  Train period: 2024-01-13 19:00:00+00:00 to 2025-05-18 15:00:00+00:00\n",
      "  Test period:  2025-05-18 15:00:00+00:00 to 2025-06-30 07:00:00+00:00\n",
      "Train set: 544,976 rows\n",
      "Test set: 136,245 rows\n",
      "\n",
      "============================================================\n",
      "Training Random Forest for dropoffs\n",
      "============================================================\n",
      "\n",
      "Model parameters:\n",
      "  - Trees: 100\n",
      "  - Max depth: 20\n",
      "  - Min samples split: 20\n",
      "  - Min samples leaf: 10\n",
      "  - Max features: sqrt\n",
      "  - Parallel jobs: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   23.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Training completed in 31.03 seconds\n",
      "  OOB Score (R¬≤): 0.3812\n",
      "\n",
      "Evaluating dropoffs model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DROPOFFS Model Performance:\n",
      "==================================================\n",
      "  MAE:  0.50\n",
      "  RMSE: 0.73\n",
      "  R¬≤:   0.3539\n",
      "  MAPE: 29.30%\n",
      "  Prediction time: 0.42s for 136,245 samples\n",
      "\n",
      "‚úì Feature importance plot saved as 'feature_importance_dropoffs.png'\n",
      "‚úì Prediction plots saved as 'predictions_dropoffs.png'\n",
      "\n",
      "‚úì Model saved as 'rf_model_dropoffs.joblib'\n",
      "\n",
      "============================================================\n",
      "NET FLOW ANALYSIS (Dropoffs - Pickups)\n",
      "============================================================\n",
      "\n",
      "Net Flow Prediction Performance:\n",
      "  MAE:  0.53\n",
      "  RMSE: 0.82\n",
      "  R¬≤:   0.6187\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SUMMARY OF RESULTS\n",
      "============================================================\n",
      "\n",
      "PICKUPS:\n",
      "  MAE: 0.21\n",
      "  RMSE: 0.44\n",
      "  R¬≤: 0.7613\n",
      "\n",
      "DROPOFFS:\n",
      "  MAE: 0.50\n",
      "  RMSE: 0.73\n",
      "  R¬≤: 0.3539\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# FOR RAW TRIP DATA (like yours with ride_id, start_time, etc.)\n",
    "# ========================================================================\n",
    "pipeline = BlueBikesRFPipeline(\n",
    "    pickle_path='../data_pipeline/data/processed/bluebikes/after_duplicates.pkl',\n",
    "    sample_size=0.8,  # Use 10% of data for faster processing\n",
    "    random_state=42,\n",
    "    needs_preprocessing=True,  # Set to True for raw trip data\n",
    "    train_start_date='2023-12-31',  # Jan 1\n",
    "    train_end_date='2025-06-30'     # June 30\n",
    ")\n",
    "\n",
    "# Run full pipeline\n",
    "results = pipeline.run_full_pipeline(\n",
    "    train_pickups=True,\n",
    "    train_dropoffs=True\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for target, metrics in results.items():\n",
    "    print(f\"\\n{target.upper()}:\")\n",
    "    print(f\"  MAE: {metrics['mae']:.2f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"  R¬≤: {metrics['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314041c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate trained model on any specified month/year with proper handling to avoid data leakage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_path='rf_model_pickups.joblib',\n",
    "                 data_pickle_path='../data_pipeline/data/processed/bluebikes/station_id_mapping.pkl',\n",
    "                 eval_year=2025,\n",
    "                 eval_month=10,\n",
    "                 train_end_date=None,\n",
    "                 train_sample_size=0.1):\n",
    "        \"\"\"\n",
    "        Initialize evaluator\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str\n",
    "            Path to saved model (.joblib file)\n",
    "        data_pickle_path : str\n",
    "            Path to raw data pickle file\n",
    "        eval_year : int\n",
    "            Year for evaluation (e.g., 2025)\n",
    "        eval_month : int\n",
    "            Month for evaluation (1-12)\n",
    "        train_end_date : str\n",
    "            Last date used for TRAINING THE MODEL (e.g., '2025-06-30')\n",
    "            This is the cutoff date your model was trained up to\n",
    "            IMPORTANT: Must match the train_end_date used in training!\n",
    "        train_sample_size : float or int or None\n",
    "            Sample size for historical data (for speed):\n",
    "            - float (0.0-1.0): fraction of training data (e.g., 0.1 = 10%)\n",
    "            - int: exact number of rows\n",
    "            - None: use all training data (slow!)\n",
    "            Evaluation month is ALWAYS evaluated in full (no sampling)\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.data_pickle_path = data_pickle_path\n",
    "        self.eval_year = eval_year\n",
    "        self.eval_month = eval_month\n",
    "        self.train_sample_size = train_sample_size\n",
    "        \n",
    "        # Calculate month boundaries (timezone-aware UTC to match data)\n",
    "        self.test_start_date = pd.to_datetime(f'{eval_year}-{eval_month:02d}-01', utc=True)\n",
    "        last_day = monthrange(eval_year, eval_month)[1]\n",
    "        self.test_end_date = pd.to_datetime(f'{eval_year}-{eval_month:02d}-{last_day} 23:59:59', utc=True)\n",
    "        \n",
    "        # Set train_end_date (CRITICAL: must match training!)\n",
    "        if train_end_date is None:\n",
    "            # Default: one day before eval month starts\n",
    "            self.train_end_date = self.test_start_date - pd.Timedelta(days=1)\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: train_end_date not specified!\")\n",
    "            print(f\"   Defaulting to: {self.train_end_date.date()}\")\n",
    "            print(f\"   Make sure this matches your model training cutoff!\\n\")\n",
    "        else:\n",
    "            self.train_end_date = pd.to_datetime(train_end_date, utc=True)\n",
    "        \n",
    "        self.model = None\n",
    "        self.raw_data = None\n",
    "        self.results = {}\n",
    "        \n",
    "        # Month name for display\n",
    "        month_names = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                      'July', 'August', 'September', 'October', 'November', 'December']\n",
    "        self.month_name = month_names[eval_month - 1]\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MODEL EVALUATION INITIALIZED - {self.month_name.upper()} {eval_year}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Model: {model_path}\")\n",
    "        print(f\"Data: {data_pickle_path}\")\n",
    "        print(f\"Model trained up to: {self.train_end_date.date()}\")\n",
    "        print(f\"Historical data sample: {train_sample_size if train_sample_size else 'ALL (no sampling)'}\")\n",
    "        print(f\"Evaluation period: {self.test_start_date.date()} to {self.test_end_date.date()}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load saved model\"\"\"\n",
    "        print(\"Loading saved model...\")\n",
    "        self.model = joblib.load(self.model_path)\n",
    "        print(f\"‚úì Model loaded: {type(self.model).__name__}\")\n",
    "        print(f\"  Features: {len(self.model.feature_names_in_)}\")\n",
    "        print(f\"  Estimators: {self.model.n_estimators if hasattr(self.model, 'n_estimators') else 'N/A'}\")\n",
    "        return self.model\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load raw data from pickle\"\"\"\n",
    "        print(\"\\nLoading raw data...\")\n",
    "        with open(self.data_pickle_path, 'rb') as f:\n",
    "            self.raw_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"‚úì Data loaded: {self.raw_data.shape[0]:,} rows\")\n",
    "        \n",
    "        # Ensure datetime columns are timezone-aware (UTC)\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.raw_data['start_time']):\n",
    "            self.raw_data['start_time'] = pd.to_datetime(self.raw_data['start_time'], utc=True)\n",
    "        elif self.raw_data['start_time'].dt.tz is None:\n",
    "            # Add timezone if missing\n",
    "            self.raw_data['start_time'] = self.raw_data['start_time'].dt.tz_localize('UTC')\n",
    "            \n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.raw_data['stop_time']):\n",
    "            self.raw_data['stop_time'] = pd.to_datetime(self.raw_data['stop_time'], utc=True)\n",
    "        elif self.raw_data['stop_time'].dt.tz is None:\n",
    "            # Add timezone if missing\n",
    "            self.raw_data['stop_time'] = self.raw_data['stop_time'].dt.tz_localize('UTC')\n",
    "        \n",
    "        # Show date range\n",
    "        print(f\"  Date range: {self.raw_data['start_time'].min()} to {self.raw_data['start_time'].max()}\")\n",
    "        \n",
    "        return self.raw_data\n",
    "    \n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess raw trip data into station-hour aggregates with features\n",
    "        \"\"\"\n",
    "        print(f\"\\nPreprocessing data ({len(df):,} trips)...\")\n",
    "        \n",
    "        # Step 1: Aggregate pickups\n",
    "        pickups = df.groupby([\n",
    "            pd.Grouper(key='start_time', freq='1H'),\n",
    "            'start_station_id'\n",
    "        ]).agg({\n",
    "            'ride_id': 'count',\n",
    "            'start_station_latitude': 'first',\n",
    "            'start_station_longitude': 'first',\n",
    "            'user_type': lambda x: (x == 'member').sum() / len(x) if len(x) > 0 else 0,\n",
    "        }).reset_index()\n",
    "        \n",
    "        pickups.columns = ['time', 'station_id', 'pickups', 'latitude', \n",
    "                          'longitude', 'member_ratio']\n",
    "        \n",
    "        # Step 2: Aggregate dropoffs\n",
    "        dropoffs = df.groupby([\n",
    "            pd.Grouper(key='stop_time', freq='1H'),\n",
    "            'end_station_id'\n",
    "        ]).agg({\n",
    "            'ride_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        dropoffs.columns = ['time', 'station_id', 'dropoffs']\n",
    "        \n",
    "        # Step 3: Merge\n",
    "        station_demand = pd.merge(\n",
    "            pickups, \n",
    "            dropoffs, \n",
    "            on=['time', 'station_id'], \n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        station_demand['net_flow'] = station_demand['dropoffs'] - station_demand['pickups']\n",
    "        \n",
    "        # Step 4: Temporal features\n",
    "        station_demand['hour'] = station_demand['time'].dt.hour\n",
    "        station_demand['day_of_week'] = station_demand['time'].dt.dayofweek\n",
    "        station_demand['month'] = station_demand['time'].dt.month\n",
    "        station_demand['day_of_year'] = station_demand['time'].dt.dayofyear\n",
    "        station_demand['week_of_year'] = station_demand['time'].dt.isocalendar().week\n",
    "        \n",
    "        station_demand['is_weekend'] = (station_demand['day_of_week'] >= 5).astype(int)\n",
    "        station_demand['is_rush_hour_morning'] = station_demand['hour'].isin([7, 8, 9]).astype(int)\n",
    "        station_demand['is_rush_hour_evening'] = station_demand['hour'].isin([17, 18, 19]).astype(int)\n",
    "        station_demand['is_business_hours'] = station_demand['hour'].between(9, 17).astype(int)\n",
    "        station_demand['is_night'] = ((station_demand['hour'] >= 22) | \n",
    "                                      (station_demand['hour'] <= 5)).astype(int)\n",
    "        \n",
    "        station_demand['hour_sin'] = np.sin(2 * np.pi * station_demand['hour'] / 24)\n",
    "        station_demand['hour_cos'] = np.cos(2 * np.pi * station_demand['hour'] / 24)\n",
    "        station_demand['dow_sin'] = np.sin(2 * np.pi * station_demand['day_of_week'] / 7)\n",
    "        station_demand['dow_cos'] = np.cos(2 * np.pi * station_demand['day_of_week'] / 7)\n",
    "        \n",
    "        # Step 5: Lag features\n",
    "        station_demand = station_demand.sort_values(['station_id', 'time'])\n",
    "        \n",
    "        for lag in [1, 24, 168]:\n",
    "            station_demand[f'pickups_lag_{lag}h'] = station_demand.groupby('station_id')['pickups'].shift(lag)\n",
    "            station_demand[f'dropoffs_lag_{lag}h'] = station_demand.groupby('station_id')['dropoffs'].shift(lag)\n",
    "        \n",
    "        # Step 6: Rolling features\n",
    "        for window in [24, 168]:\n",
    "            station_demand[f'pickups_rolling_{window}h'] = (\n",
    "                station_demand.groupby('station_id')['pickups']\n",
    "                .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "            station_demand[f'dropoffs_rolling_{window}h'] = (\n",
    "                station_demand.groupby('station_id')['dropoffs']\n",
    "                .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            )\n",
    "        \n",
    "        # Step 7: Station statistics\n",
    "        station_stats = station_demand.groupby('station_id').agg({\n",
    "            'pickups': ['mean', 'std', 'max'],\n",
    "            'dropoffs': ['mean', 'std', 'max'],\n",
    "        })\n",
    "        station_stats.columns = ['_'.join(col) for col in station_stats.columns]\n",
    "        station_stats = station_stats.reset_index()\n",
    "        \n",
    "        station_demand = station_demand.merge(station_stats, on='station_id', how='left')\n",
    "        \n",
    "        # Step 8: Filter inactive stations\n",
    "        station_total_activity = station_demand.groupby('station_id')[['pickups', 'dropoffs']].sum()\n",
    "        active_stations = station_total_activity[\n",
    "            (station_total_activity['pickups'] > 10) | \n",
    "            (station_total_activity['dropoffs'] > 10)\n",
    "        ].index\n",
    "        \n",
    "        station_demand = station_demand[station_demand['station_id'].isin(active_stations)]\n",
    "        \n",
    "        # Drop NaN\n",
    "        station_demand = station_demand.dropna()\n",
    "        \n",
    "        print(f\"‚úì Preprocessed: {len(station_demand):,} station-hour records\")\n",
    "        print(f\"  Active stations: {len(active_stations)}\")\n",
    "        print(f\"  Date range: {station_demand['time'].min()} to {station_demand['time'].max()}\")\n",
    "        \n",
    "        return station_demand\n",
    "    \n",
    "    def prepare_evaluation_data(self):\n",
    "        \"\"\"\n",
    "        Prepare evaluation data ensuring no data leakage\n",
    "        CRITICAL: Lag features must use only pre-evaluation data!\n",
    "        Uses sampling for training data (speed) but full evaluation month\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"PREPARING EVALUATION DATA (AVOIDING DATA LEAKAGE)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Split raw data\n",
    "        train_raw = self.raw_data[self.raw_data['start_time'] <= self.train_end_date].copy()\n",
    "        test_raw = self.raw_data[\n",
    "            (self.raw_data['start_time'] >= self.test_start_date) & \n",
    "            (self.raw_data['start_time'] <= self.test_end_date)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"\\nRaw data split:\")\n",
    "        print(f\"  Training trips (full): {len(train_raw):,} (up to {self.train_end_date.date()})\")\n",
    "        print(f\"  Evaluation trips: {len(test_raw):,} ({self.test_start_date.date()} to {self.test_end_date.date()})\")\n",
    "        \n",
    "        if len(test_raw) == 0:\n",
    "            raise ValueError(f\"No data found for {self.month_name} {self.eval_year}! \"\n",
    "                           f\"Your data range: {self.raw_data['start_time'].min()} to {self.raw_data['start_time'].max()}\")\n",
    "        \n",
    "        # OPTIMIZATION: Sample training data for speed (but keep ALL evaluation data)\n",
    "        if self.train_sample_size is not None:\n",
    "            if isinstance(self.train_sample_size, float):\n",
    "                n_samples = int(len(train_raw) * self.train_sample_size)\n",
    "            else:\n",
    "                n_samples = self.train_sample_size\n",
    "            \n",
    "            print(f\"\\n SPEED OPTIMIZATION: Sampling training data\")\n",
    "            print(f\"  Training trips (sampled): {n_samples:,} ({self.train_sample_size*100 if isinstance(self.train_sample_size, float) else 'N/A'}%)\")\n",
    "            print(f\"  Evaluation trips (full): {len(test_raw):,} (NO sampling for evaluation!)\")\n",
    "            \n",
    "            # Sample training data randomly but maintain temporal distribution\n",
    "            train_raw_sampled = train_raw.sample(n=min(n_samples, len(train_raw)), \n",
    "                                                 random_state=42)\n",
    "        else:\n",
    "            train_raw_sampled = train_raw\n",
    "            print(f\"\\n  Using ALL training data (no sampling)\")\n",
    "        \n",
    "        # CRITICAL: Combine sampled train + full test BEFORE preprocessing\n",
    "        # This ensures lag features for evaluation month use historical data\n",
    "        combined_raw = pd.concat([train_raw_sampled, test_raw], ignore_index=True)\n",
    "        \n",
    "        print(f\"\\nCombined for lag computation: {len(combined_raw):,} trips\")\n",
    "        print(f\"  = {len(train_raw_sampled):,} training + {len(test_raw):,} evaluation\")\n",
    "        \n",
    "        # Preprocess combined data\n",
    "        combined_processed = self.preprocess_data(combined_raw)\n",
    "        \n",
    "        # Extract ONLY evaluation month rows\n",
    "        eval_processed = combined_processed[\n",
    "            (combined_processed['time'] >= self.test_start_date) &\n",
    "            (combined_processed['time'] <= self.test_end_date)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"\\n‚úì {self.month_name} data ready: {len(eval_processed):,} station-hour records\")\n",
    "        print(f\"  Stations: {eval_processed['station_id'].nunique()}\")\n",
    "        print(f\"  Hours covered: {(eval_processed['time'].max() - eval_processed['time'].min()).days * 24}\")\n",
    "        \n",
    "        # Verify no data leakage in lag features\n",
    "        print(f\"\\nVerifying lag features use only pre-{self.month_name} data...\")\n",
    "        sample_first = eval_processed[eval_processed['time'] == self.test_start_date].head(1)\n",
    "        if len(sample_first) > 0:\n",
    "            lag_168_val = sample_first['pickups_lag_168h'].values[0]\n",
    "            lag_date = self.test_start_date - pd.Timedelta(days=7)\n",
    "            print(f\"  Example: {self.month_name} 1st lag_168h = {lag_168_val:.2f} \"\n",
    "                  f\"(from {lag_date.date()}, 7 days prior) ‚úì\")\n",
    "        \n",
    "        return eval_processed\n",
    "    \n",
    "    def compute_metrics(self, actuals, predictions):\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        mape = np.mean(np.abs((actuals - predictions) / (actuals + 1))) * 100\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'n_samples': len(actuals)\n",
    "        }\n",
    "    \n",
    "    def predict_and_evaluate(self, eval_data):\n",
    "        \"\"\"Make predictions and compute overall metrics\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"MAKING PREDICTIONS AND EVALUATING\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Get features in correct order\n",
    "        X = eval_data[self.model.feature_names_in_]\n",
    "        \n",
    "        #  AUTO-DETECT TARGET from model filename\n",
    "        if 'dropoff' in self.model_path.lower():\n",
    "            target_col = 'dropoffs'\n",
    "        elif 'pickup' in self.model_path.lower():\n",
    "            target_col = 'pickups'\n",
    "        elif 'net_flow' in self.model_path.lower():\n",
    "            target_col = 'net_flow'\n",
    "        else:\n",
    "            # Default to pickups with warning\n",
    "            target_col = 'pickups'\n",
    "            print(\"  Warning: Could not detect target from model filename, defaulting to 'pickups'\")\n",
    "        \n",
    "        y_actual = eval_data[target_col]\n",
    "        \n",
    "        print(f\"Target variable: {target_col}\")\n",
    "        print(f\"Predicting on {len(X):,} station-hours...\")\n",
    "        y_pred = self.model.predict(X)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results_df = pd.DataFrame({\n",
    "            'time': eval_data['time'],\n",
    "            'station_id': eval_data['station_id'],\n",
    "            'hour': eval_data['hour'],\n",
    "            'day_of_week': eval_data['day_of_week'],\n",
    "            'actual': y_actual,\n",
    "            'predicted': y_pred,\n",
    "            'error': y_actual - y_pred,\n",
    "            'abs_error': np.abs(y_actual - y_pred)\n",
    "        })\n",
    "            \n",
    "        # Overall metrics\n",
    "        overall_metrics = self.compute_metrics(y_actual, y_pred)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"OVERALL METRICS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Samples:      {overall_metrics['n_samples']:,}\")\n",
    "        print(f\"MAE:          {overall_metrics['mae']:.3f}\")\n",
    "        print(f\"RMSE:         {overall_metrics['rmse']:.3f}\")\n",
    "        print(f\"R¬≤:           {overall_metrics['r2']:.4f}\")\n",
    "        print(f\"MAPE:         {overall_metrics['mape']:.2f}%\")\n",
    "        \n",
    "        return results_df, overall_metrics\n",
    "    \n",
    "    def temporal_analysis(self, results_df):\n",
    "        \"\"\"Analyze performance by time dimensions\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"TEMPORAL ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # By hour\n",
    "        by_hour = results_df.groupby('hour').apply(\n",
    "            lambda x: pd.Series(self.compute_metrics(x['actual'], x['predicted']))\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nPerformance by Hour of Day:\")\n",
    "        print(f\"{'Hour':<6} {'MAE':<8} {'R¬≤':<8} {'Samples':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for hour in sorted(by_hour.index):\n",
    "            metrics = by_hour.loc[hour]\n",
    "            print(f\"{hour:02d}:00  {metrics['mae']:6.3f}   {metrics['r2']:6.4f}   {int(metrics['n_samples']):,}\")\n",
    "        \n",
    "        # By day of week\n",
    "        by_dow = results_df.groupby('day_of_week').apply(\n",
    "            lambda x: pd.Series(self.compute_metrics(x['actual'], x['predicted']))\n",
    "        )\n",
    "        \n",
    "        dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        \n",
    "        print(f\"\\nPerformance by Day of Week:\")\n",
    "        print(f\"{'Day':<10} {'MAE':<8} {'R¬≤':<8} {'Samples':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for dow in sorted(by_dow.index):\n",
    "            metrics = by_dow.loc[dow]\n",
    "            print(f\"{dow_names[dow]:<10} {metrics['mae']:6.3f}   {metrics['r2']:6.4f}   {int(metrics['n_samples']):,}\")\n",
    "        \n",
    "        # By week\n",
    "        results_df['week'] = results_df['time'].dt.isocalendar().week\n",
    "        by_week = results_df.groupby('week').apply(\n",
    "            lambda x: pd.Series(self.compute_metrics(x['actual'], x['predicted']))\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nPerformance by Week:\")\n",
    "        print(f\"{'Week':<6} {'MAE':<8} {'R¬≤':<8} {'Samples':<10}\")\n",
    "        print(\"-\" * 40)\n",
    "        for week in sorted(by_week.index):\n",
    "            metrics = by_week.loc[week]\n",
    "            print(f\"{int(week):02d}     {metrics['mae']:6.3f}   {metrics['r2']:6.4f}   {int(metrics['n_samples']):,}\")\n",
    "        \n",
    "        return {\n",
    "            'by_hour': by_hour,\n",
    "            'by_dow': by_dow,\n",
    "            'by_week': by_week\n",
    "        }\n",
    "    \n",
    "    def station_analysis(self, results_df):\n",
    "        \"\"\"Analyze performance by station\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"STATION ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        by_station = results_df.groupby('station_id').apply(\n",
    "            lambda x: pd.Series({\n",
    "                **self.compute_metrics(x['actual'], x['predicted']),\n",
    "                'mean_actual': x['actual'].mean(),\n",
    "                'mean_predicted': x['predicted'].mean(),\n",
    "                'bias': x['predicted'].mean() - x['actual'].mean()\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # Worst stations\n",
    "        worst_stations = by_station.nlargest(10, 'mae')\n",
    "        print(f\"\\nWORST 10 STATIONS (Highest MAE):\")\n",
    "        print(f\"{'Station':<10} {'MAE':<8} {'R¬≤':<8} {'Bias':<8} {'Samples':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        for station_id in worst_stations.index:\n",
    "            metrics = worst_stations.loc[station_id]\n",
    "            print(f\"{int(station_id):<10} {metrics['mae']:6.3f}   {metrics['r2']:6.4f}   \"\n",
    "                  f\"{metrics['bias']:+6.3f}   {int(metrics['n_samples']):,}\")\n",
    "        \n",
    "        # Best stations\n",
    "        best_stations = by_station.nsmallest(10, 'mae')\n",
    "        print(f\"\\nBEST 10 STATIONS (Lowest MAE):\")\n",
    "        print(f\"{'Station':<10} {'MAE':<8} {'R¬≤':<8} {'Bias':<8} {'Samples':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        for station_id in best_stations.index:\n",
    "            metrics = best_stations.loc[station_id]\n",
    "            print(f\"{int(station_id):<10} {metrics['mae']:6.3f}   {metrics['r2']:6.4f}   \"\n",
    "                  f\"{metrics['bias']:+6.3f}   {int(metrics['n_samples']):,}\")\n",
    "        \n",
    "        return {\n",
    "            'by_station': by_station,\n",
    "            'worst': worst_stations,\n",
    "            'best': best_stations\n",
    "        }\n",
    "    \n",
    "    def create_visualizations(self, results_df):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"CREATING VISUALIZATIONS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Time series - Daily average\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        daily = results_df.groupby(results_df['time'].dt.date).agg({\n",
    "            'actual': 'mean',\n",
    "            'predicted': 'mean'\n",
    "        })\n",
    "        ax1.plot(daily.index, daily['actual'], label='Actual', linewidth=2, alpha=0.8)\n",
    "        ax1.plot(daily.index, daily['predicted'], label='Predicted', \n",
    "                linewidth=2, linestyle='--', alpha=0.8)\n",
    "        ax1.set_title(f'Daily Average Pickups per Station-Hour - {self.month_name} {self.eval_year}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Average Pickups')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Actual vs Predicted scatter\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        sample_size = min(1000, len(results_df))\n",
    "        sample = results_df.sample(n=sample_size, random_state=42)\n",
    "        ax2.scatter(sample['actual'], sample['predicted'], alpha=0.5, s=10)\n",
    "        max_val = max(sample['actual'].max(), sample['predicted'].max())\n",
    "        ax2.plot([0, max_val], [0, max_val], 'r--', lw=2, label='Perfect prediction')\n",
    "        ax2.set_xlabel('Actual')\n",
    "        ax2.set_ylabel('Predicted')\n",
    "        ax2.set_title('Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Residuals distribution\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        ax3.hist(results_df['error'], bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "        ax3.set_xlabel('Residual (Actual - Predicted)')\n",
    "        ax3.set_ylabel('Frequency')\n",
    "        ax3.set_title('Residuals Distribution', fontsize=12, fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. MAE by hour\n",
    "        ax4 = fig.add_subplot(gs[1, 2])\n",
    "        hourly_mae = results_df.groupby('hour')['abs_error'].mean()\n",
    "        ax4.bar(hourly_mae.index, hourly_mae.values, color='steelblue', alpha=0.7)\n",
    "        ax4.set_xlabel('Hour of Day')\n",
    "        ax4.set_ylabel('MAE')\n",
    "        ax4.set_title('MAE by Hour', fontsize=12, fontweight='bold')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 5. MAE by day of week\n",
    "        ax5 = fig.add_subplot(gs[2, 0])\n",
    "        dow_mae = results_df.groupby('day_of_week')['abs_error'].mean()\n",
    "        dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        ax5.bar(range(7), dow_mae.values, color='coral', alpha=0.7)\n",
    "        ax5.set_xticks(range(7))\n",
    "        ax5.set_xticklabels(dow_names)\n",
    "        ax5.set_ylabel('MAE')\n",
    "        ax5.set_title('MAE by Day of Week', fontsize=12, fontweight='bold')\n",
    "        ax5.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 6. Error heatmap (hour x day of week)\n",
    "        ax6 = fig.add_subplot(gs[2, 1:])\n",
    "        error_pivot = results_df.pivot_table(\n",
    "            values='abs_error',\n",
    "            index='hour',\n",
    "            columns='day_of_week',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        sns.heatmap(error_pivot, annot=False, fmt='.2f', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'MAE'}, ax=ax6)\n",
    "        ax6.set_xticklabels(dow_names)\n",
    "        ax6.set_ylabel('Hour of Day')\n",
    "        ax6.set_xlabel('Day of Week')\n",
    "        ax6.set_title('MAE Heatmap (Hour √ó Day)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle(f'{self.month_name} {self.eval_year} Model Evaluation', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        filename = f'evaluation_{self.eval_year}_{self.eval_month:02d}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Saved: {filename}\")\n",
    "        \n",
    "        # Additional plot: Station performance distribution\n",
    "        fig2, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Station MAE distribution\n",
    "        station_mae = results_df.groupby('station_id')['abs_error'].mean()\n",
    "        axes[0].hist(station_mae, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        axes[0].axvline(x=station_mae.median(), color='r', linestyle='--', \n",
    "                       lw=2, label=f'Median: {station_mae.median():.3f}')\n",
    "        axes[0].set_xlabel('MAE per Station')\n",
    "        axes[0].set_ylabel('Number of Stations')\n",
    "        axes[0].set_title('Distribution of Station-Level MAE', fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Station R¬≤ distribution\n",
    "        station_r2 = results_df.groupby('station_id').apply(\n",
    "            lambda x: r2_score(x['actual'], x['predicted']) if len(x) > 1 else 0\n",
    "        )\n",
    "        axes[1].hist(station_r2, bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "        axes[1].axvline(x=station_r2.median(), color='r', linestyle='--', \n",
    "                       lw=2, label=f'Median: {station_r2.median():.3f}')\n",
    "        axes[1].set_xlabel('R¬≤ per Station')\n",
    "        axes[1].set_ylabel('Number of Stations')\n",
    "        axes[1].set_title('Distribution of Station-Level R¬≤', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename2 = f'evaluation_{self.eval_year}_{self.eval_month:02d}_stations.png'\n",
    "        plt.savefig(filename2, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úì Saved: {filename2}\")\n",
    "    \n",
    "    def generate_summary_report(self, overall_metrics, temporal_metrics, station_metrics):\n",
    "        \"\"\"Generate text summary report\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"GENERATING SUMMARY REPORT\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        report = f\"\"\"\n",
    "{'='*70}\n",
    "{self.month_name.upper()} {self.eval_year} MODEL EVALUATION SUMMARY REPORT\n",
    "{'='*70}\n",
    "\n",
    "Model: {self.model_path}\n",
    "Evaluation Period: {self.test_start_date.date()} to {self.test_end_date.date()}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*70}\n",
    "1. OVERALL PERFORMANCE\n",
    "{'='*70}\n",
    "\n",
    "Total Predictions: {overall_metrics['n_samples']:,}\n",
    "MAE:               {overall_metrics['mae']:.3f} pickups/hour\n",
    "RMSE:              {overall_metrics['rmse']:.3f} pickups/hour\n",
    "R¬≤:                {overall_metrics['r2']:.4f}\n",
    "MAPE:              {overall_metrics['mape']:.2f}%\n",
    "\n",
    "Interpretation:\n",
    "- On average, predictions are off by {overall_metrics['mae']:.2f} pickups per hour\n",
    "- Model explains {overall_metrics['r2']*100:.1f}% of variance in demand\n",
    "- Typical prediction error is {overall_metrics['mape']:.1f}%\n",
    "\n",
    "{'='*70}\n",
    "2. TEMPORAL PERFORMANCE\n",
    "{'='*70}\n",
    "\n",
    "Best Hours (Lowest MAE):\n",
    "\"\"\"\n",
    "        best_hours = temporal_metrics['by_hour'].nsmallest(3, 'mae')\n",
    "        for hour in best_hours.index:\n",
    "            report += f\"  {hour:02d}:00 - MAE: {best_hours.loc[hour, 'mae']:.3f}, R¬≤: {best_hours.loc[hour, 'r2']:.4f}\\n\"\n",
    "        \n",
    "        report += \"\\nWorst Hours (Highest MAE):\\n\"\n",
    "        worst_hours = temporal_metrics['by_hour'].nlargest(3, 'mae')\n",
    "        for hour in worst_hours.index:\n",
    "            report += f\"  {hour:02d}:00 - MAE: {worst_hours.loc[hour, 'mae']:.3f}, R¬≤: {worst_hours.loc[hour, 'r2']:.4f}\\n\"\n",
    "        \n",
    "        dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        report += \"\\nPerformance by Day:\\n\"\n",
    "        for dow in range(7):\n",
    "            metrics = temporal_metrics['by_dow'].loc[dow]\n",
    "            report += f\"  {dow_names[dow]:<10} - MAE: {metrics['mae']:.3f}, R¬≤: {metrics['r2']:.4f}\\n\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "{'='*70}\n",
    "3. STATION PERFORMANCE\n",
    "{'='*70}\n",
    "\n",
    "Total Stations Evaluated: {len(station_metrics['by_station'])}\n",
    "\n",
    "Top 5 Best Stations (Lowest MAE):\n",
    "\"\"\"\n",
    "        best_5 = station_metrics['best'].head(5)\n",
    "        for station_id in best_5.index:\n",
    "            metrics = best_5.loc[station_id]\n",
    "            report += f\"  Station {int(station_id):>4} - MAE: {metrics['mae']:.3f}, R¬≤: {metrics['r2']:.4f}\\n\"\n",
    "        \n",
    "        report += \"\\nTop 5 Worst Stations (Highest MAE):\\n\"\n",
    "        worst_5 = station_metrics['worst'].head(5)\n",
    "        for station_id in worst_5.index:\n",
    "            metrics = worst_5.loc[station_id]\n",
    "            report += f\"  Station {int(station_id):>4} - MAE: {metrics['mae']:.3f}, R¬≤: {metrics['r2']:.4f}\\n\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "{'='*70}\n",
    "4. KEY FINDINGS & RECOMMENDATIONS\n",
    "{'='*70}\n",
    "\n",
    "Performance Assessment:\n",
    "\"\"\"\n",
    "        if overall_metrics['r2'] > 0.80:\n",
    "            report += \"‚úì EXCELLENT - Model performance exceeds industry standards\\n\"\n",
    "        elif overall_metrics['r2'] > 0.70:\n",
    "            report += \"‚úì GOOD - Model performance meets industry standards\\n\"\n",
    "        elif overall_metrics['r2'] > 0.60:\n",
    "            report += \"‚ö† FAIR - Model performance is acceptable but could be improved\\n\"\n",
    "        else:\n",
    "            report += \"‚ö† NEEDS IMPROVEMENT - Model performance below expectations\\n\"\n",
    "        \n",
    "        # Check for weekday vs weekend performance\n",
    "        weekday_r2 = temporal_metrics['by_dow'].loc[0:4, 'r2'].mean()\n",
    "        weekend_r2 = temporal_metrics['by_dow'].loc[5:6, 'r2'].mean()\n",
    "        \n",
    "        if weekday_r2 - weekend_r2 > 0.1:\n",
    "            report += \"\\n‚ö† Weekend Performance Issue:\\n\"\n",
    "            report += f\"  Weekday R¬≤: {weekday_r2:.3f}\\n\"\n",
    "            report += f\"  Weekend R¬≤: {weekend_r2:.3f}\\n\"\n",
    "            report += \"  Recommendation: Consider weekend-specific model or features\\n\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "{'='*70}\n",
    "EVALUATION COMPLETE\n",
    "{'='*70}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save report to file\n",
    "        filename = f'evaluation_{self.eval_year}_{self.eval_month:02d}_report.txt'\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(report)\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "    \n",
    "    def run_full_evaluation(self):\n",
    "        \"\"\"Execute complete evaluation pipeline\"\"\"\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# STARTING FULL {self.month_name.upper()} {self.eval_year} EVALUATION\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Step 1: Load model and data\n",
    "        self.load_model()\n",
    "        self.load_data()\n",
    "        \n",
    "        # Step 2: Prepare evaluation data (avoiding data leakage)\n",
    "        eval_data = self.prepare_evaluation_data()\n",
    "        \n",
    "        # Step 3: Make predictions and compute overall metrics\n",
    "        results_df, overall_metrics = self.predict_and_evaluate(eval_data)\n",
    "        \n",
    "        # Step 4: Temporal analysis\n",
    "        temporal_metrics = self.temporal_analysis(results_df)\n",
    "        \n",
    "        # Step 5: Station analysis\n",
    "        station_metrics = self.station_analysis(results_df)\n",
    "        \n",
    "        # Step 6: Create visualizations\n",
    "        self.create_visualizations(results_df)\n",
    "        \n",
    "        # Step 7: Generate summary report\n",
    "        self.generate_summary_report(overall_metrics, temporal_metrics, station_metrics)\n",
    "        \n",
    "        # Store results\n",
    "        self.results = {\n",
    "            'overall': overall_metrics,\n",
    "            'temporal': temporal_metrics,\n",
    "            'stations': station_metrics,\n",
    "            'predictions': results_df\n",
    "        }\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# EVALUATION COMPLETED IN {elapsed:.1f} SECONDS\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        return self.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec016ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION INITIALIZED - JULY 2025\n",
      "======================================================================\n",
      "Model: rf_model_pickups.joblib\n",
      "Data: ../data_pipeline/data/processed/bluebikes/after_duplicates.pkl\n",
      "Model trained up to: 2025-06-30\n",
      "Historical data sample: 0.8\n",
      "Evaluation period: 2025-07-01 to 2025-07-31\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING FULL JULY 2025 EVALUATION\n",
      "######################################################################\n",
      "\n",
      "Loading saved model...\n",
      "‚úì Model loaded: RandomForestRegressor\n",
      "  Features: 33\n",
      "  Estimators: 100\n",
      "\n",
      "Loading raw data...\n",
      "‚úì Data loaded: 2,084,559 rows\n",
      "  Date range: 2024-01-01 00:05:08+00:00 to 2025-10-31 23:56:01.017000+00:00\n",
      "\n",
      "======================================================================\n",
      "PREPARING EVALUATION DATA (AVOIDING DATA LEAKAGE)\n",
      "======================================================================\n",
      "\n",
      "Raw data split:\n",
      "  Training trips (full): 831,372 (up to 2025-06-30)\n",
      "  Evaluation trips: 134,574 (2025-07-01 to 2025-07-31)\n",
      "\n",
      "‚ö° SPEED OPTIMIZATION: Sampling training data\n",
      "  Training trips (sampled): 665,097 (80.0%)\n",
      "  Evaluation trips (full): 134,574 (NO sampling for evaluation!)\n",
      "\n",
      "Combined for lag computation: 799,671 trips\n",
      "  = 665,097 training + 134,574 evaluation\n",
      "\n",
      "Preprocessing data (799,671 trips)...\n",
      "‚úì Preprocessed: 805,697 station-hour records\n",
      "  Active stations: 582\n",
      "  Date range: 2024-01-13 19:00:00+00:00 to 2025-08-01 00:00:00+00:00\n",
      "\n",
      "‚úì July data ready: 124,473 station-hour records\n",
      "  Stations: 490\n",
      "  Hours covered: 720\n",
      "\n",
      "Verifying lag features use only pre-July data...\n",
      "  Example: July 1st lag_168h = 0.00 (from 2025-06-24, 7 days prior) ‚úì\n",
      "\n",
      "======================================================================\n",
      "MAKING PREDICTIONS AND EVALUATING\n",
      "======================================================================\n",
      "\n",
      "Predicting on 124,473 station-hours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OVERALL METRICS\n",
      "======================================================================\n",
      "Samples:      124,473\n",
      "MAE:          0.325\n",
      "RMSE:         0.630\n",
      "R¬≤:           0.7192\n",
      "MAPE:         11.44%\n",
      "\n",
      "======================================================================\n",
      "TEMPORAL ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Performance by Hour of Day:\n",
      "Hour   MAE      R¬≤       Samples   \n",
      "----------------------------------------\n",
      "00:00   0.146   0.8057   2,393\n",
      "01:00   0.162   0.7919   1,620\n",
      "02:00   0.108   0.8458   1,023\n",
      "03:00   0.082   0.9092   504\n",
      "04:00   0.065   0.9422   420\n",
      "05:00   0.093   0.8659   1,351\n",
      "06:00   0.166   0.8048   3,239\n",
      "07:00   0.307   0.6736   5,375\n",
      "08:00   0.445   0.5985   6,987\n",
      "09:00   0.308   0.6979   6,626\n",
      "10:00   0.252   0.7472   5,777\n",
      "11:00   0.262   0.7515   5,688\n",
      "12:00   0.299   0.7360   6,482\n",
      "13:00   0.303   0.7265   6,618\n",
      "14:00   0.298   0.7451   6,803\n",
      "15:00   0.336   0.7215   7,439\n",
      "16:00   0.413   0.7135   8,531\n",
      "17:00   0.498   0.6974   9,275\n",
      "18:00   0.438   0.7142   8,920\n",
      "19:00   0.367   0.7236   8,140\n",
      "20:00   0.313   0.7230   6,924\n",
      "21:00   0.272   0.7250   5,745\n",
      "22:00   0.275   0.6168   4,929\n",
      "23:00   0.190   0.7582   3,664\n",
      "\n",
      "Performance by Day of Week:\n",
      "Day        MAE      R¬≤       Samples   \n",
      "----------------------------------------\n",
      "Monday      0.327   0.7234   16,366\n",
      "Tuesday     0.337   0.7149   20,489\n",
      "Wednesday   0.335   0.7242   20,778\n",
      "Thursday    0.318   0.7166   18,457\n",
      "Friday      0.338   0.6876   17,035\n",
      "Saturday    0.323   0.7285   16,612\n",
      "Sunday      0.290   0.7494   14,736\n",
      "\n",
      "Performance by Week:\n",
      "Week   MAE      R¬≤       Samples   \n",
      "----------------------------------------\n",
      "27      0.313   0.6906   22,624\n",
      "28      0.324   0.7300   27,485\n",
      "29      0.331   0.7302   29,300\n",
      "30      0.331   0.7244   29,431\n",
      "31      0.323   0.7057   15,633\n",
      "\n",
      "======================================================================\n",
      "STATION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "WORST 10 STATIONS (Highest MAE):\n",
      "Station    MAE      R¬≤       Bias     Samples   \n",
      "--------------------------------------------------\n",
      "182         1.246   0.5862   -0.149   582\n",
      "1           1.102   0.6295   -0.078   468\n",
      "27          1.063   0.6004   -0.240   573\n",
      "39          0.951   0.5981   -0.103   586\n",
      "28          0.937   0.4502   -0.063   540\n",
      "530         0.934   0.5743   -0.116   590\n",
      "68          0.895   0.5751   -0.085   560\n",
      "426         0.793   0.5074   -0.003   555\n",
      "204         0.753   0.6390   -0.123   552\n",
      "353         0.745   0.5832   -0.060   550\n",
      "\n",
      "BEST 10 STATIONS (Lowest MAE):\n",
      "Station    MAE      R¬≤       Bias     Samples   \n",
      "--------------------------------------------------\n",
      "413         0.002   0.0000   +0.002   2\n",
      "342         0.016   0.9975   +0.016   39\n",
      "205         0.021   0.9963   +0.021   26\n",
      "407         0.022   0.9964   +0.022   64\n",
      "533         0.023   0.9961   +0.023   7\n",
      "478         0.025   0.9946   +0.025   34\n",
      "338         0.025   0.9940   +0.025   70\n",
      "474         0.025   0.9947   +0.025   47\n",
      "62          0.026   0.9955   +0.026   25\n",
      "565         0.027   0.9947   +0.027   15\n",
      "\n",
      "======================================================================\n",
      "CREATING VISUALIZATIONS\n",
      "======================================================================\n",
      "‚úì Saved: evaluation_2025_07.png\n",
      "‚úì Saved: evaluation_2025_07_stations.png\n",
      "\n",
      "======================================================================\n",
      "GENERATING SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "JULY 2025 MODEL EVALUATION SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "Model: rf_model_pickups.joblib\n",
      "Evaluation Period: 2025-07-01 to 2025-07-31\n",
      "Generated: 2025-11-13 14:53:03\n",
      "\n",
      "======================================================================\n",
      "1. OVERALL PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Total Predictions: 124,473\n",
      "MAE:               0.325 pickups/hour\n",
      "RMSE:              0.630 pickups/hour\n",
      "R¬≤:                0.7192\n",
      "MAPE:              11.44%\n",
      "\n",
      "Interpretation:\n",
      "- On average, predictions are off by 0.33 pickups per hour\n",
      "- Model explains 71.9% of variance in demand\n",
      "- Typical prediction error is 11.4%\n",
      "\n",
      "======================================================================\n",
      "2. TEMPORAL PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Best Hours (Lowest MAE):\n",
      "  04:00 - MAE: 0.065, R¬≤: 0.9422\n",
      "  03:00 - MAE: 0.082, R¬≤: 0.9092\n",
      "  05:00 - MAE: 0.093, R¬≤: 0.8659\n",
      "\n",
      "Worst Hours (Highest MAE):\n",
      "  17:00 - MAE: 0.498, R¬≤: 0.6974\n",
      "  08:00 - MAE: 0.445, R¬≤: 0.5985\n",
      "  18:00 - MAE: 0.438, R¬≤: 0.7142\n",
      "\n",
      "Performance by Day:\n",
      "  Monday     - MAE: 0.327, R¬≤: 0.7234\n",
      "  Tuesday    - MAE: 0.337, R¬≤: 0.7149\n",
      "  Wednesday  - MAE: 0.335, R¬≤: 0.7242\n",
      "  Thursday   - MAE: 0.318, R¬≤: 0.7166\n",
      "  Friday     - MAE: 0.338, R¬≤: 0.6876\n",
      "  Saturday   - MAE: 0.323, R¬≤: 0.7285\n",
      "  Sunday     - MAE: 0.290, R¬≤: 0.7494\n",
      "\n",
      "======================================================================\n",
      "3. STATION PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Total Stations Evaluated: 490\n",
      "\n",
      "Top 5 Best Stations (Lowest MAE):\n",
      "  Station  413 - MAE: 0.002, R¬≤: 0.0000\n",
      "  Station  342 - MAE: 0.016, R¬≤: 0.9975\n",
      "  Station  205 - MAE: 0.021, R¬≤: 0.9963\n",
      "  Station  407 - MAE: 0.022, R¬≤: 0.9964\n",
      "  Station  533 - MAE: 0.023, R¬≤: 0.9961\n",
      "\n",
      "Top 5 Worst Stations (Highest MAE):\n",
      "  Station  182 - MAE: 1.246, R¬≤: 0.5862\n",
      "  Station    1 - MAE: 1.102, R¬≤: 0.6295\n",
      "  Station   27 - MAE: 1.063, R¬≤: 0.6004\n",
      "  Station   39 - MAE: 0.951, R¬≤: 0.5981\n",
      "  Station   28 - MAE: 0.937, R¬≤: 0.4502\n",
      "\n",
      "======================================================================\n",
      "4. KEY FINDINGS & RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "Performance Assessment:\n",
      "‚úì GOOD - Model performance meets industry standards\n",
      "\n",
      "======================================================================\n",
      "EVALUATION COMPLETE\n",
      "======================================================================\n",
      "        \n",
      "\n",
      "‚úì Saved: evaluation_2025_07_report.txt\n",
      "\n",
      "######################################################################\n",
      "# EVALUATION COMPLETED IN 69.2 SECONDS\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "RESULTS STORED IN:\n",
      "======================================================================\n",
      "1. results['overall']     - Overall metrics dictionary\n",
      "2. results['temporal']    - Performance by hour/day/week\n",
      "3. results['stations']    - Performance by station\n",
      "4. results['predictions'] - Full predictions DataFrame\n",
      "\n",
      "Files generated:\n",
      "- evaluation_2025_07.png\n",
      "- evaluation_2025_07_stations.png\n",
      "- evaluation_2025_07_report.txt\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "QUICK RESULTS ACCESS:\n",
      "======================================================================\n",
      "Overall R¬≤: 0.7192\n",
      "Overall MAE: 0.325\n",
      "\n",
      "Best performing hour: 04:00\n",
      "Worst performing hour: 17:00\n",
      "\n",
      "Best station: 413\n",
      "Worst station: 182\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Option 1: Evaluate October 2025 (default)\n",
    "evaluator = ModelEvaluator(\n",
    "    model_path='rf_model_pickups.joblib',\n",
    "    data_pickle_path='../data_pipeline/data/processed/bluebikes/after_duplicates.pkl',\n",
    "    eval_year=2025,\n",
    "    eval_month=7,  # October\n",
    "    train_sample_size=0.8,\n",
    "    train_end_date='2025-06-30' \n",
    ")\n",
    "\n",
    "results = evaluator.run_full_evaluation()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS STORED IN:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. results['overall']     - Overall metrics dictionary\")\n",
    "print(\"2. results['temporal']    - Performance by hour/day/week\")\n",
    "print(\"3. results['stations']    - Performance by station\")\n",
    "print(\"4. results['predictions'] - Full predictions DataFrame\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(f\"- evaluation_{evaluator.eval_year}_{evaluator.eval_month:02d}.png\")\n",
    "print(f\"- evaluation_{evaluator.eval_year}_{evaluator.eval_month:02d}_stations.png\")\n",
    "print(f\"- evaluation_{evaluator.eval_year}_{evaluator.eval_month:02d}_report.txt\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================================================\n",
    "# EXAMPLE: Access specific results\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK RESULTS ACCESS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Overall R¬≤: {results['overall']['r2']:.4f}\")\n",
    "print(f\"Overall MAE: {results['overall']['mae']:.3f}\")\n",
    "print(f\"\\nBest performing hour: {results['temporal']['by_hour']['mae'].idxmin():02d}:00\")\n",
    "print(f\"Worst performing hour: {results['temporal']['by_hour']['mae'].idxmax():02d}:00\")\n",
    "print(f\"\\nBest station: {int(results['stations']['best'].index[0])}\")\n",
    "print(f\"Worst station: {int(results['stations']['worst'].index[0])}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f42aaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION INITIALIZED - JULY 2025\n",
      "======================================================================\n",
      "Model: rf_model_dropoffs.joblib\n",
      "Data: ../data_pipeline/data/processed/bluebikes/after_duplicates.pkl\n",
      "Model trained up to: 2025-06-30\n",
      "Historical data sample: 0.8\n",
      "Evaluation period: 2025-07-01 to 2025-07-31\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING FULL JULY 2025 EVALUATION\n",
      "######################################################################\n",
      "\n",
      "Loading saved model...\n",
      "‚úì Model loaded: RandomForestRegressor\n",
      "  Features: 33\n",
      "  Estimators: 100\n",
      "\n",
      "Loading raw data...\n",
      "‚úì Data loaded: 2,084,559 rows\n",
      "  Date range: 2024-01-01 00:05:08+00:00 to 2025-10-31 23:56:01.017000+00:00\n",
      "\n",
      "======================================================================\n",
      "PREPARING EVALUATION DATA (AVOIDING DATA LEAKAGE)\n",
      "======================================================================\n",
      "\n",
      "Raw data split:\n",
      "  Training trips (full): 831,372 (up to 2025-06-30)\n",
      "  Evaluation trips: 134,574 (2025-07-01 to 2025-07-31)\n",
      "\n",
      "‚ö° SPEED OPTIMIZATION: Sampling training data\n",
      "  Training trips (sampled): 665,097 (80.0%)\n",
      "  Evaluation trips (full): 134,574 (NO sampling for evaluation!)\n",
      "\n",
      "Combined for lag computation: 799,671 trips\n",
      "  = 665,097 training + 134,574 evaluation\n",
      "\n",
      "Preprocessing data (799,671 trips)...\n",
      "‚úì Preprocessed: 805,697 station-hour records\n",
      "  Active stations: 582\n",
      "  Date range: 2024-01-13 19:00:00+00:00 to 2025-08-01 00:00:00+00:00\n",
      "\n",
      "‚úì July data ready: 124,473 station-hour records\n",
      "  Stations: 490\n",
      "  Hours covered: 720\n",
      "\n",
      "Verifying lag features use only pre-July data...\n",
      "  Example: July 1st lag_168h = 0.00 (from 2025-06-24, 7 days prior) ‚úì\n",
      "\n",
      "======================================================================\n",
      "MAKING PREDICTIONS AND EVALUATING\n",
      "======================================================================\n",
      "Target variable: dropoffs\n",
      "Predicting on 124,473 station-hours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OVERALL METRICS\n",
      "======================================================================\n",
      "Samples:      124,473\n",
      "MAE:          0.660\n",
      "RMSE:         0.962\n",
      "R¬≤:           0.3584\n",
      "MAPE:         35.91%\n",
      "\n",
      "======================================================================\n",
      "TEMPORAL ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Performance by Hour of Day:\n",
      "Hour   MAE      R¬≤       Samples   \n",
      "----------------------------------------\n",
      "00:00   0.370   0.3718   2,393\n",
      "01:00   0.374   0.3512   1,620\n",
      "02:00   0.320   0.4680   1,023\n",
      "03:00   0.256   0.5528   504\n",
      "04:00   0.214   0.6709   420\n",
      "05:00   0.229   0.6678   1,351\n",
      "06:00   0.413   0.3877   3,239\n",
      "07:00   0.586   0.3363   5,375\n",
      "08:00   0.809   0.3719   6,987\n",
      "09:00   0.701   0.3796   6,626\n",
      "10:00   0.598   0.3465   5,777\n",
      "11:00   0.594   0.3157   5,688\n",
      "12:00   0.634   0.3111   6,482\n",
      "13:00   0.629   0.3055   6,618\n",
      "14:00   0.643   0.3109   6,803\n",
      "15:00   0.667   0.3112   7,439\n",
      "16:00   0.739   0.3522   8,531\n",
      "17:00   0.854   0.3803   9,275\n",
      "18:00   0.837   0.3467   8,920\n",
      "19:00   0.750   0.3164   8,140\n",
      "20:00   0.684   0.2767   6,924\n",
      "21:00   0.618   0.2364   5,745\n",
      "22:00   0.558   0.2223   4,929\n",
      "23:00   0.483   0.2873   3,664\n",
      "\n",
      "Performance by Day of Week:\n",
      "Day        MAE      R¬≤       Samples   \n",
      "----------------------------------------\n",
      "Monday      0.663   0.3625   16,366\n",
      "Tuesday     0.672   0.3729   20,489\n",
      "Wednesday   0.668   0.3795   20,778\n",
      "Thursday    0.660   0.3450   18,457\n",
      "Friday      0.670   0.3383   17,035\n",
      "Saturday    0.661   0.3446   16,612\n",
      "Sunday      0.617   0.3485   14,736\n",
      "\n",
      "Performance by Week:\n",
      "Week   MAE      R¬≤       Samples   \n",
      "----------------------------------------\n",
      "27      0.642   0.3327   22,624\n",
      "28      0.658   0.3706   27,485\n",
      "29      0.669   0.3621   29,300\n",
      "30      0.669   0.3604   29,431\n",
      "31      0.657   0.3592   15,633\n",
      "\n",
      "======================================================================\n",
      "STATION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "WORST 10 STATIONS (Highest MAE):\n",
      "Station    MAE      R¬≤       Bias     Samples   \n",
      "--------------------------------------------------\n",
      "1           1.915   0.3471   -0.286   468\n",
      "182         1.586   0.3702   -0.160   582\n",
      "28          1.486   0.2276   -0.111   540\n",
      "27          1.402   0.3801   -0.188   573\n",
      "530         1.376   0.3238   -0.248   590\n",
      "39          1.352   0.3271   -0.154   586\n",
      "231         1.328   0.3188   +0.011   454\n",
      "426         1.311   0.1879   -0.073   555\n",
      "204         1.242   0.3014   -0.089   552\n",
      "301         1.163   0.2354   -0.045   519\n",
      "\n",
      "BEST 10 STATIONS (Lowest MAE):\n",
      "Station    MAE      R¬≤       Bias     Samples   \n",
      "--------------------------------------------------\n",
      "413         0.023   0.0000   +0.023   2\n",
      "533         0.078   0.9716   +0.078   7\n",
      "509         0.081   0.9625   +0.081   14\n",
      "565         0.081   0.9597   +0.081   15\n",
      "296         0.095   0.8774   +0.044   34\n",
      "66          0.097   0.8283   +0.011   41\n",
      "478         0.100   0.8861   +0.054   34\n",
      "342         0.102   0.8783   +0.059   39\n",
      "558         0.103   0.9424   +0.103   6\n",
      "434         0.106   0.9022   +0.074   60\n",
      "\n",
      "======================================================================\n",
      "CREATING VISUALIZATIONS\n",
      "======================================================================\n",
      "‚úì Saved: evaluation_2025_07.png\n",
      "‚úì Saved: evaluation_2025_07_stations.png\n",
      "\n",
      "======================================================================\n",
      "GENERATING SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "JULY 2025 MODEL EVALUATION SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "Model: rf_model_dropoffs.joblib\n",
      "Evaluation Period: 2025-07-01 to 2025-07-31\n",
      "Generated: 2025-11-13 15:38:00\n",
      "\n",
      "======================================================================\n",
      "1. OVERALL PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Total Predictions: 124,473\n",
      "MAE:               0.660 pickups/hour\n",
      "RMSE:              0.962 pickups/hour\n",
      "R¬≤:                0.3584\n",
      "MAPE:              35.91%\n",
      "\n",
      "Interpretation:\n",
      "- On average, predictions are off by 0.66 pickups per hour\n",
      "- Model explains 35.8% of variance in demand\n",
      "- Typical prediction error is 35.9%\n",
      "\n",
      "======================================================================\n",
      "2. TEMPORAL PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Best Hours (Lowest MAE):\n",
      "  04:00 - MAE: 0.214, R¬≤: 0.6709\n",
      "  05:00 - MAE: 0.229, R¬≤: 0.6678\n",
      "  03:00 - MAE: 0.256, R¬≤: 0.5528\n",
      "\n",
      "Worst Hours (Highest MAE):\n",
      "  17:00 - MAE: 0.854, R¬≤: 0.3803\n",
      "  18:00 - MAE: 0.837, R¬≤: 0.3467\n",
      "  08:00 - MAE: 0.809, R¬≤: 0.3719\n",
      "\n",
      "Performance by Day:\n",
      "  Monday     - MAE: 0.663, R¬≤: 0.3625\n",
      "  Tuesday    - MAE: 0.672, R¬≤: 0.3729\n",
      "  Wednesday  - MAE: 0.668, R¬≤: 0.3795\n",
      "  Thursday   - MAE: 0.660, R¬≤: 0.3450\n",
      "  Friday     - MAE: 0.670, R¬≤: 0.3383\n",
      "  Saturday   - MAE: 0.661, R¬≤: 0.3446\n",
      "  Sunday     - MAE: 0.617, R¬≤: 0.3485\n",
      "\n",
      "======================================================================\n",
      "3. STATION PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Total Stations Evaluated: 490\n",
      "\n",
      "Top 5 Best Stations (Lowest MAE):\n",
      "  Station  413 - MAE: 0.023, R¬≤: 0.0000\n",
      "  Station  533 - MAE: 0.078, R¬≤: 0.9716\n",
      "  Station  509 - MAE: 0.081, R¬≤: 0.9625\n",
      "  Station  565 - MAE: 0.081, R¬≤: 0.9597\n",
      "  Station  296 - MAE: 0.095, R¬≤: 0.8774\n",
      "\n",
      "Top 5 Worst Stations (Highest MAE):\n",
      "  Station    1 - MAE: 1.915, R¬≤: 0.3471\n",
      "  Station  182 - MAE: 1.586, R¬≤: 0.3702\n",
      "  Station   28 - MAE: 1.486, R¬≤: 0.2276\n",
      "  Station   27 - MAE: 1.402, R¬≤: 0.3801\n",
      "  Station  530 - MAE: 1.376, R¬≤: 0.3238\n",
      "\n",
      "======================================================================\n",
      "4. KEY FINDINGS & RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "Performance Assessment:\n",
      "‚ö† NEEDS IMPROVEMENT - Model performance below expectations\n",
      "\n",
      "======================================================================\n",
      "EVALUATION COMPLETE\n",
      "======================================================================\n",
      "        \n",
      "\n",
      "‚úì Saved: evaluation_2025_07_report.txt\n",
      "\n",
      "######################################################################\n",
      "# EVALUATION COMPLETED IN 96.7 SECONDS\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "RESULTS STORED IN:\n",
      "======================================================================\n",
      "1. results['overall']     - Overall metrics dictionary\n",
      "2. results['temporal']    - Performance by hour/day/week\n",
      "3. results['stations']    - Performance by station\n",
      "4. results['predictions'] - Full predictions DataFrame\n",
      "\n",
      "Files generated:\n",
      "- evaluation_2025_07.png\n",
      "- evaluation_2025_07_stations.png\n",
      "- evaluation_2025_07_report.txt\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "QUICK RESULTS ACCESS:\n",
      "======================================================================\n",
      "Overall R¬≤: 0.3584\n",
      "Overall MAE: 0.660\n",
      "\n",
      "Best performing hour: 04:00\n",
      "Worst performing hour: 17:00\n",
      "\n",
      "Best station: 413\n",
      "Worst station: 1\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "evaluator = ModelEvaluator(\n",
    "    model_path='rf_model_dropoffs.joblib',  # ‚Üê Dropoffs model\n",
    "    data_pickle_path='../data_pipeline/data/processed/bluebikes/after_duplicates.pkl',\n",
    "    eval_year=2025,\n",
    "    eval_month=7,\n",
    "    train_sample_size=0.8,\n",
    "    train_end_date='2025-06-30' \n",
    ")\n",
    "\n",
    "# ========================================================================\n",
    "# RUN EVALUATION\n",
    "# ========================================================================\n",
    "\n",
    "results = evaluator.run_full_evaluation()\n",
    "\n",
    "# ========================================================================\n",
    "# ACCESS RESULTS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS STORED IN:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. results['overall']     - Overall metrics dictionary\")\n",
    "print(\"2. results['temporal']    - Performance by hour/day/week\")\n",
    "print(\"3. results['stations']    - Performance by station\")\n",
    "print(\"4. results['predictions'] - Full predictions DataFrame\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(f\"- evaluation_{evaluator.eval_year}_{evaluator.eval_month:02d}.png\")\n",
    "print(f\"- evaluation_{evaluator.eval_year}_{evaluator.eval_month:02d}_stations.png\")\n",
    "print(f\"- evaluation_{evaluator.eval_year}_{evaluator.eval_month:02d}_report.txt\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================================================\n",
    "# EXAMPLE: Access specific results\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK RESULTS ACCESS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Overall R¬≤: {results['overall']['r2']:.4f}\")\n",
    "print(f\"Overall MAE: {results['overall']['mae']:.3f}\")\n",
    "print(f\"\\nBest performing hour: {results['temporal']['by_hour']['mae'].idxmin():02d}:00\")\n",
    "print(f\"Worst performing hour: {results['temporal']['by_hour']['mae'].idxmax():02d}:00\")\n",
    "print(f\"\\nBest station: {int(results['stations']['best'].index[0])}\")\n",
    "print(f\"Worst station: {int(results['stations']['worst'].index[0])}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f568100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RebalancingOptimizer:\n",
    "    \"\"\"\n",
    "    Complete bike rebalancing system using trained demand prediction models.\n",
    "    Predicts future demand and optimizes truck routes to prevent stockouts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 pickup_model_path='rf_model_pickups.joblib',\n",
    "                 dropoff_model_path='rf_model_dropoffs.joblib',\n",
    "                 station_capacity=20,\n",
    "                 min_bikes_threshold=3,\n",
    "                 max_bikes_threshold=17,\n",
    "                 prediction_horizon=8):\n",
    "        \"\"\"\n",
    "        Initialize rebalancing optimizer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pickup_model_path : str\n",
    "            Path to trained pickup prediction model\n",
    "        dropoff_model_path : str\n",
    "            Path to trained dropoff prediction model\n",
    "        station_capacity : int\n",
    "            Maximum bikes a station can hold\n",
    "        min_bikes_threshold : int\n",
    "            Minimum bikes before considered \"low\"\n",
    "        max_bikes_threshold : int\n",
    "            Maximum bikes before considered \"full\"\n",
    "        prediction_horizon : int\n",
    "            Number of hours to predict ahead (default: 8)\n",
    "        \"\"\"\n",
    "        self.pickup_model = joblib.load(pickup_model_path)\n",
    "        self.dropoff_model = joblib.load(dropoff_model_path)\n",
    "        \n",
    "        self.station_capacity = station_capacity\n",
    "        self.min_bikes_threshold = min_bikes_threshold\n",
    "        self.max_bikes_threshold = max_bikes_threshold\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"REBALANCING OPTIMIZER INITIALIZED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Pickup model loaded: {pickup_model_path}\")\n",
    "        print(f\"Dropoff model loaded: {dropoff_model_path}\")\n",
    "        print(f\"Station capacity: {station_capacity} bikes\")\n",
    "        print(f\"Thresholds: Low={min_bikes_threshold}, High={max_bikes_threshold}\")\n",
    "        print(f\"Prediction horizon: {prediction_horizon} hours\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def prepare_features_for_prediction(self, target_time, stations_data, historical_data):\n",
    "        \"\"\"\n",
    "        Prepare features for future time prediction\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_time : datetime\n",
    "            Time to predict for\n",
    "        stations_data : DataFrame\n",
    "            Station information (id, lat, lon, etc.)\n",
    "        historical_data : DataFrame\n",
    "            Recent historical data for computing lag/rolling features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with features for all stations at target_time\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "        \n",
    "        for _, station in stations_data.iterrows():\n",
    "            station_id = station['station_id']\n",
    "            \n",
    "            # Get historical data for this station\n",
    "            station_hist = historical_data[\n",
    "                historical_data['station_id'] == station_id\n",
    "            ].sort_values('time')\n",
    "            \n",
    "            # Temporal features\n",
    "            hour = target_time.hour\n",
    "            day_of_week = target_time.dayofweek\n",
    "            month = target_time.month\n",
    "            day_of_year = target_time.dayofyear\n",
    "            week_of_year = target_time.isocalendar()[1]\n",
    "            \n",
    "            # Binary features\n",
    "            is_weekend = 1 if day_of_week >= 5 else 0\n",
    "            is_rush_hour_morning = 1 if hour in [7, 8, 9] else 0\n",
    "            is_rush_hour_evening = 1 if hour in [17, 18, 19] else 0\n",
    "            is_business_hours = 1 if 9 <= hour <= 17 else 0\n",
    "            is_night = 1 if hour >= 22 or hour <= 5 else 0\n",
    "            \n",
    "            # Cyclical encoding\n",
    "            hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "            hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "            dow_sin = np.sin(2 * np.pi * day_of_week / 7)\n",
    "            dow_cos = np.cos(2 * np.pi * day_of_week / 7)\n",
    "            \n",
    "            # Lag features (get from historical data)\n",
    "            try:\n",
    "                pickups_lag_1h = station_hist.iloc[-1]['pickups'] if len(station_hist) > 0 else 0\n",
    "                pickups_lag_24h = station_hist.iloc[-24]['pickups'] if len(station_hist) >= 24 else pickups_lag_1h\n",
    "                pickups_lag_168h = station_hist.iloc[-168]['pickups'] if len(station_hist) >= 168 else pickups_lag_24h\n",
    "                \n",
    "                dropoffs_lag_1h = station_hist.iloc[-1]['dropoffs'] if len(station_hist) > 0 else 0\n",
    "                dropoffs_lag_24h = station_hist.iloc[-24]['dropoffs'] if len(station_hist) >= 24 else dropoffs_lag_1h\n",
    "                dropoffs_lag_168h = station_hist.iloc[-168]['dropoffs'] if len(station_hist) >= 168 else dropoffs_lag_24h\n",
    "            except:\n",
    "                pickups_lag_1h = pickups_lag_24h = pickups_lag_168h = 0\n",
    "                dropoffs_lag_1h = dropoffs_lag_24h = dropoffs_lag_168h = 0\n",
    "            \n",
    "            # Rolling features\n",
    "            if len(station_hist) >= 24:\n",
    "                pickups_rolling_24h = station_hist.tail(24)['pickups'].mean()\n",
    "                dropoffs_rolling_24h = station_hist.tail(24)['dropoffs'].mean()\n",
    "            else:\n",
    "                pickups_rolling_24h = pickups_lag_1h\n",
    "                dropoffs_rolling_24h = dropoffs_lag_1h\n",
    "            \n",
    "            if len(station_hist) >= 168:\n",
    "                pickups_rolling_168h = station_hist.tail(168)['pickups'].mean()\n",
    "                dropoffs_rolling_168h = station_hist.tail(168)['dropoffs'].mean()\n",
    "            else:\n",
    "                pickups_rolling_168h = pickups_rolling_24h\n",
    "                dropoffs_rolling_168h = dropoffs_rolling_24h\n",
    "            \n",
    "            # Station statistics\n",
    "            pickups_mean = station.get('pickups_mean', station_hist['pickups'].mean() if len(station_hist) > 0 else 0)\n",
    "            pickups_std = station.get('pickups_std', station_hist['pickups'].std() if len(station_hist) > 0 else 0)\n",
    "            pickups_max = station.get('pickups_max', station_hist['pickups'].max() if len(station_hist) > 0 else 0)\n",
    "            dropoffs_mean = station.get('dropoffs_mean', station_hist['dropoffs'].mean() if len(station_hist) > 0 else 0)\n",
    "            dropoffs_std = station.get('dropoffs_std', station_hist['dropoffs'].std() if len(station_hist) > 0 else 0)\n",
    "            dropoffs_max = station.get('dropoffs_max', station_hist['dropoffs'].max() if len(station_hist) > 0 else 0)\n",
    "            \n",
    "            # Compile features\n",
    "            features = {\n",
    "                'latitude': station['latitude'],\n",
    "                'longitude': station['longitude'],\n",
    "                'member_ratio': station.get('member_ratio', 0.5),\n",
    "                'hour': hour,\n",
    "                'day_of_week': day_of_week,\n",
    "                'month': month,\n",
    "                'day_of_year': day_of_year,\n",
    "                'week_of_year': week_of_year,\n",
    "                'is_weekend': is_weekend,\n",
    "                'is_rush_hour_morning': is_rush_hour_morning,\n",
    "                'is_rush_hour_evening': is_rush_hour_evening,\n",
    "                'is_business_hours': is_business_hours,\n",
    "                'is_night': is_night,\n",
    "                'hour_sin': hour_sin,\n",
    "                'hour_cos': hour_cos,\n",
    "                'dow_sin': dow_sin,\n",
    "                'dow_cos': dow_cos,\n",
    "                'pickups_lag_1h': pickups_lag_1h,\n",
    "                'pickups_lag_24h': pickups_lag_24h,\n",
    "                'pickups_lag_168h': pickups_lag_168h,\n",
    "                'dropoffs_lag_1h': dropoffs_lag_1h,\n",
    "                'dropoffs_lag_24h': dropoffs_lag_24h,\n",
    "                'dropoffs_lag_168h': dropoffs_lag_168h,\n",
    "                'pickups_rolling_24h': pickups_rolling_24h,\n",
    "                'pickups_rolling_168h': pickups_rolling_168h,\n",
    "                'dropoffs_rolling_24h': dropoffs_rolling_24h,\n",
    "                'dropoffs_rolling_168h': dropoffs_rolling_168h,\n",
    "                'pickups_mean': pickups_mean,\n",
    "                'pickups_std': pickups_std,\n",
    "                'pickups_max': pickups_max,\n",
    "                'dropoffs_mean': dropoffs_mean,\n",
    "                'dropoffs_std': dropoffs_std,\n",
    "                'dropoffs_max': dropoffs_max,\n",
    "            }\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        return pd.DataFrame(features_list)\n",
    "    \n",
    "    def predict_demand(self, target_time, stations_data, historical_data):\n",
    "        \"\"\"\n",
    "        Predict pickups and dropoffs for all stations at target time\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with predictions for each station\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        features_df = self.prepare_features_for_prediction(target_time, stations_data, historical_data)\n",
    "        \n",
    "        # Ensure feature order matches model training\n",
    "        X_pickups = features_df[self.pickup_model.feature_names_in_]\n",
    "        X_dropoffs = features_df[self.dropoff_model.feature_names_in_]\n",
    "        \n",
    "        # Predict\n",
    "        pickups_pred = self.pickup_model.predict(X_pickups)\n",
    "        dropoffs_pred = self.dropoff_model.predict(X_dropoffs)\n",
    "        \n",
    "        # Create predictions dataframe\n",
    "        predictions = pd.DataFrame({\n",
    "            'station_id': stations_data['station_id'],\n",
    "            'time': target_time,\n",
    "            'predicted_pickups': pickups_pred,\n",
    "            'predicted_dropoffs': dropoffs_pred,\n",
    "            'predicted_net_flow': dropoffs_pred - pickups_pred,\n",
    "            'latitude': stations_data['latitude'],\n",
    "            'longitude': stations_data['longitude']\n",
    "        })\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_next_hours(self, current_time, stations_data, historical_data, hours=None):\n",
    "        \"\"\"\n",
    "        Predict demand for next N hours\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        current_time : datetime\n",
    "            Current time\n",
    "        stations_data : DataFrame\n",
    "            Station information\n",
    "        historical_data : DataFrame\n",
    "            Recent historical aggregated data\n",
    "        hours : int, optional\n",
    "            Number of hours to predict (default: self.prediction_horizon)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with predictions for all stations and hours\n",
    "        \"\"\"\n",
    "        if hours is None:\n",
    "            hours = self.prediction_horizon\n",
    "        \n",
    "        print(f\"\\nPredicting demand for next {hours} hours...\")\n",
    "        print(f\"Starting from: {current_time}\")\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        for hour_offset in range(1, hours + 1):\n",
    "            target_time = current_time + timedelta(hours=hour_offset)\n",
    "            \n",
    "            # Predict for this hour\n",
    "            hour_predictions = self.predict_demand(target_time, stations_data, historical_data)\n",
    "            all_predictions.append(hour_predictions)\n",
    "            \n",
    "            # Update historical data with prediction (for next iteration's lag features)\n",
    "            # This is a simplification - in production, use actual data as it becomes available\n",
    "            new_record = hour_predictions[['station_id', 'time', 'predicted_pickups', 'predicted_dropoffs']].copy()\n",
    "            new_record.columns = ['station_id', 'time', 'pickups', 'dropoffs']\n",
    "            historical_data = pd.concat([historical_data, new_record], ignore_index=True)\n",
    "        \n",
    "        predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "        \n",
    "        print(f\"‚úì Predictions complete: {len(predictions_df):,} station-hour predictions\")\n",
    "        \n",
    "        return predictions_df\n",
    "    \n",
    "    def simulate_inventory(self, predictions, current_inventory):\n",
    "        \"\"\"\n",
    "        Simulate bike inventory over prediction horizon\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : DataFrame\n",
    "            Predicted pickups/dropoffs for all stations and hours\n",
    "        current_inventory : dict\n",
    "            Current bike count at each station {station_id: count}\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with projected inventory over time\n",
    "        \"\"\"\n",
    "        print(\"\\nSimulating inventory over time...\")\n",
    "        \n",
    "        inventory_timeline = []\n",
    "        \n",
    "        # Group by station\n",
    "        for station_id in predictions['station_id'].unique():\n",
    "            station_preds = predictions[predictions['station_id'] == station_id].sort_values('time')\n",
    "            \n",
    "            # Initialize inventory\n",
    "            current_bikes = current_inventory.get(station_id, 10)  # Default: 10 bikes\n",
    "            \n",
    "            for _, row in station_preds.iterrows():\n",
    "                # Update inventory based on predicted net flow\n",
    "                current_bikes += row['predicted_net_flow']\n",
    "                \n",
    "                # Clip to capacity\n",
    "                current_bikes = max(0, min(current_bikes, self.station_capacity))\n",
    "                \n",
    "                inventory_timeline.append({\n",
    "                    'station_id': station_id,\n",
    "                    'time': row['time'],\n",
    "                    'predicted_bikes': current_bikes,\n",
    "                    'predicted_pickups': row['predicted_pickups'],\n",
    "                    'predicted_dropoffs': row['predicted_dropoffs'],\n",
    "                    'predicted_net_flow': row['predicted_net_flow']\n",
    "                })\n",
    "        \n",
    "        inventory_df = pd.DataFrame(inventory_timeline)\n",
    "        \n",
    "        print(f\"‚úì Inventory simulation complete\")\n",
    "        \n",
    "        return inventory_df\n",
    "    \n",
    "    def identify_rebalancing_needs(self, inventory_timeline):\n",
    "        \"\"\"\n",
    "        Identify stations that will need rebalancing\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inventory_timeline : DataFrame\n",
    "            Projected inventory from simulate_inventory()\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with rebalancing needs\n",
    "        \"\"\"\n",
    "        print(\"\\nIdentifying rebalancing needs...\")\n",
    "        \n",
    "        needs = []\n",
    "        \n",
    "        # Group by station\n",
    "        for station_id in inventory_timeline['station_id'].unique():\n",
    "            station_timeline = inventory_timeline[\n",
    "                inventory_timeline['station_id'] == station_id\n",
    "            ].sort_values('time')\n",
    "            \n",
    "            # Check each hour\n",
    "            for idx, row in station_timeline.iterrows():\n",
    "                bikes = row['predicted_bikes']\n",
    "                time = row['time']\n",
    "                \n",
    "                # Check if station will run low on bikes\n",
    "                if bikes < self.min_bikes_threshold:\n",
    "                    deficit = self.min_bikes_threshold + 2 - bikes  # Add buffer\n",
    "                    \n",
    "                    needs.append({\n",
    "                        'station_id': station_id,\n",
    "                        'time': time,\n",
    "                        'action': 'ADD',\n",
    "                        'bikes_needed': int(np.ceil(deficit)),\n",
    "                        'current_predicted_bikes': bikes,\n",
    "                        'priority': 'CRITICAL' if bikes <= 0 else 'HIGH' if bikes <= 1 else 'MEDIUM',\n",
    "                        'hours_until': (time - inventory_timeline['time'].min()).total_seconds() / 3600\n",
    "                    })\n",
    "                    break  # Only flag first occurrence for this station\n",
    "                \n",
    "                # Check if station will become too full\n",
    "                elif bikes > self.max_bikes_threshold:\n",
    "                    surplus = bikes - (self.max_bikes_threshold - 2)  # Leave buffer\n",
    "                    \n",
    "                    needs.append({\n",
    "                        'station_id': station_id,\n",
    "                        'time': time,\n",
    "                        'action': 'REMOVE',\n",
    "                        'bikes_needed': int(np.ceil(surplus)),\n",
    "                        'current_predicted_bikes': bikes,\n",
    "                        'priority': 'CRITICAL' if bikes >= self.station_capacity else 'HIGH' if bikes >= 19 else 'MEDIUM',\n",
    "                        'hours_until': (time - inventory_timeline['time'].min()).total_seconds() / 3600\n",
    "                    })\n",
    "                    break  # Only flag first occurrence\n",
    "        \n",
    "        needs_df = pd.DataFrame(needs)\n",
    "        \n",
    "        if len(needs_df) > 0:\n",
    "            print(f\"‚úì Identified {len(needs_df)} stations needing rebalancing:\")\n",
    "            print(f\"  - ADD bikes: {len(needs_df[needs_df['action'] == 'ADD'])}\")\n",
    "            print(f\"  - REMOVE bikes: {len(needs_df[needs_df['action'] == 'REMOVE'])}\")\n",
    "            print(f\"  - CRITICAL priority: {len(needs_df[needs_df['priority'] == 'CRITICAL'])}\")\n",
    "            print(f\"  - HIGH priority: {len(needs_df[needs_df['priority'] == 'HIGH'])}\")\n",
    "            print(f\"  - MEDIUM priority: {len(needs_df[needs_df['priority'] == 'MEDIUM'])}\")\n",
    "        else:\n",
    "            print(\"‚úì No rebalancing needed - all stations within acceptable range\")\n",
    "        \n",
    "        return needs_df\n",
    "    \n",
    "    def optimize_routes(self, rebalancing_needs, stations_data, depot_location=None):\n",
    "        \"\"\"\n",
    "        Optimize truck routes to fulfill rebalancing needs\n",
    "        Uses Hungarian algorithm to match surplus and deficit stations\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rebalancing_needs : DataFrame\n",
    "            Output from identify_rebalancing_needs()\n",
    "        stations_data : DataFrame\n",
    "            Station information including coordinates\n",
    "        depot_location : tuple, optional\n",
    "            (lat, lon) of truck depot. If None, uses centroid of stations\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with optimized rebalancing routes\n",
    "        \"\"\"\n",
    "        print(\"\\nOptimizing rebalancing routes...\")\n",
    "        \n",
    "        if len(rebalancing_needs) == 0:\n",
    "            print(\"‚úì No routes needed (no rebalancing required)\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Merge station coordinates\n",
    "        needs_with_coords = rebalancing_needs.merge(\n",
    "            stations_data[['station_id', 'latitude', 'longitude']],\n",
    "            on='station_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Separate surplus and deficit stations\n",
    "        surplus_stations = needs_with_coords[needs_with_coords['action'] == 'REMOVE'].copy()\n",
    "        deficit_stations = needs_with_coords[needs_with_coords['action'] == 'ADD'].copy()\n",
    "        \n",
    "        print(f\"  Surplus stations: {len(surplus_stations)}\")\n",
    "        print(f\"  Deficit stations: {len(deficit_stations)}\")\n",
    "        \n",
    "        if len(surplus_stations) == 0 or len(deficit_stations) == 0:\n",
    "            print(\"‚ö†Ô∏è  Only one-way rebalancing needed (only surplus or only deficit)\")\n",
    "            # Handle one-way rebalancing\n",
    "            return self._handle_one_way_rebalancing(surplus_stations, deficit_stations, depot_location)\n",
    "        \n",
    "        # Calculate distance matrix\n",
    "        surplus_coords = surplus_stations[['latitude', 'longitude']].values\n",
    "        deficit_coords = deficit_stations[['latitude', 'longitude']].values\n",
    "        \n",
    "        # Haversine distance in km\n",
    "        distance_matrix = self._calculate_distance_matrix(surplus_coords, deficit_coords)\n",
    "        \n",
    "        # Solve assignment problem (minimize total distance)\n",
    "        row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "        \n",
    "        # Create routes\n",
    "        routes = []\n",
    "        for i, j in zip(row_ind, col_ind):\n",
    "            surplus_row = surplus_stations.iloc[i]\n",
    "            deficit_row = deficit_stations.iloc[j]\n",
    "            \n",
    "            # Bikes to transfer (minimum of what's available and what's needed)\n",
    "            bikes_to_move = min(surplus_row['bikes_needed'], deficit_row['bikes_needed'])\n",
    "            \n",
    "            routes.append({\n",
    "                'route_id': len(routes) + 1,\n",
    "                'from_station': int(surplus_row['station_id']),\n",
    "                'to_station': int(deficit_row['station_id']),\n",
    "                'bikes_to_move': int(bikes_to_move),\n",
    "                'distance_km': distance_matrix[i, j],\n",
    "                'from_priority': surplus_row['priority'],\n",
    "                'to_priority': deficit_row['priority'],\n",
    "                'priority': max(surplus_row['priority'], deficit_row['priority']),\n",
    "                'from_lat': surplus_row['latitude'],\n",
    "                'from_lon': surplus_row['longitude'],\n",
    "                'to_lat': deficit_row['latitude'],\n",
    "                'to_lon': deficit_row['longitude'],\n",
    "                'deadline': min(surplus_row['time'], deficit_row['time'])\n",
    "            })\n",
    "        \n",
    "        routes_df = pd.DataFrame(routes)\n",
    "        \n",
    "        # Sort by priority and distance\n",
    "        priority_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2}\n",
    "        routes_df['priority_rank'] = routes_df['priority'].map(priority_order)\n",
    "        routes_df = routes_df.sort_values(['priority_rank', 'distance_km'])\n",
    "        routes_df = routes_df.drop('priority_rank', axis=1)\n",
    "        \n",
    "        # Calculate total metrics\n",
    "        total_distance = routes_df['distance_km'].sum()\n",
    "        total_bikes = routes_df['bikes_to_move'].sum()\n",
    "        \n",
    "        print(f\"\\n‚úì Routes optimized:\")\n",
    "        print(f\"  Total routes: {len(routes_df)}\")\n",
    "        print(f\"  Total bikes to move: {total_bikes}\")\n",
    "        print(f\"  Total distance: {total_distance:.2f} km\")\n",
    "        print(f\"  Average distance per route: {total_distance / len(routes_df):.2f} km\")\n",
    "        \n",
    "        return routes_df\n",
    "    \n",
    "    def _calculate_distance_matrix(self, coords1, coords2):\n",
    "        \"\"\"\n",
    "        Calculate Haversine distance matrix between two sets of coordinates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        coords1 : array-like, shape (n, 2)\n",
    "            First set of (lat, lon) coordinates\n",
    "        coords2 : array-like, shape (m, 2)\n",
    "            Second set of (lat, lon) coordinates\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        array, shape (n, m)\n",
    "            Distance matrix in kilometers\n",
    "        \"\"\"\n",
    "        # Convert to radians\n",
    "        coords1_rad = np.radians(coords1)\n",
    "        coords2_rad = np.radians(coords2)\n",
    "        \n",
    "        # Haversine formula\n",
    "        distances = np.zeros((len(coords1), len(coords2)))\n",
    "        \n",
    "        for i, (lat1, lon1) in enumerate(coords1_rad):\n",
    "            for j, (lat2, lon2) in enumerate(coords2_rad):\n",
    "                dlat = lat2 - lat1\n",
    "                dlon = lon2 - lon1\n",
    "                \n",
    "                a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "                c = 2 * np.arcsin(np.sqrt(a))\n",
    "                \n",
    "                # Earth radius in km\n",
    "                distances[i, j] = 6371 * c\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def _handle_one_way_rebalancing(self, surplus_stations, deficit_stations, depot_location):\n",
    "        \"\"\"\n",
    "        Handle case where only surplus OR only deficit exists\n",
    "        (Bikes must come from/go to depot)\n",
    "        \"\"\"\n",
    "        if depot_location is None:\n",
    "            # Use centroid as default depot\n",
    "            if len(surplus_stations) > 0:\n",
    "                depot_location = (\n",
    "                    surplus_stations['latitude'].mean(),\n",
    "                    surplus_stations['longitude'].mean()\n",
    "                )\n",
    "            else:\n",
    "                depot_location = (\n",
    "                    deficit_stations['latitude'].mean(),\n",
    "                    deficit_stations['longitude'].mean()\n",
    "                )\n",
    "        \n",
    "        routes = []\n",
    "        \n",
    "        # If only surplus (bring bikes to depot)\n",
    "        if len(surplus_stations) > 0 and len(deficit_stations) == 0:\n",
    "            for _, station in surplus_stations.iterrows():\n",
    "                distance = self._haversine_distance(\n",
    "                    station['latitude'], station['longitude'],\n",
    "                    depot_location[0], depot_location[1]\n",
    "                )\n",
    "                \n",
    "                routes.append({\n",
    "                    'route_id': len(routes) + 1,\n",
    "                    'from_station': int(station['station_id']),\n",
    "                    'to_station': 'DEPOT',\n",
    "                    'bikes_to_move': int(station['bikes_needed']),\n",
    "                    'distance_km': distance,\n",
    "                    'priority': station['priority']\n",
    "                })\n",
    "        \n",
    "        # If only deficit (bring bikes from depot)\n",
    "        elif len(deficit_stations) > 0 and len(surplus_stations) == 0:\n",
    "            for _, station in deficit_stations.iterrows():\n",
    "                distance = self._haversine_distance(\n",
    "                    depot_location[0], depot_location[1],\n",
    "                    station['latitude'], station['longitude']\n",
    "                )\n",
    "                \n",
    "                routes.append({\n",
    "                    'route_id': len(routes) + 1,\n",
    "                    'from_station': 'DEPOT',\n",
    "                    'to_station': int(station['station_id']),\n",
    "                    'bikes_to_move': int(station['bikes_needed']),\n",
    "                    'distance_km': distance,\n",
    "                    'priority': station['priority']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(routes)\n",
    "    \n",
    "    def _haversine_distance(self, lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate Haversine distance between two points in km\"\"\"\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        return 6371 * c\n",
    "    \n",
    "    def generate_rebalancing_plan(self, current_time, stations_data, historical_data, \n",
    "                                  current_inventory, hours=None):\n",
    "        \"\"\"\n",
    "        Complete rebalancing plan generation\n",
    "        \n",
    "        This is the MAIN METHOD to call for generating rebalancing instructions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        current_time : datetime\n",
    "            Current time (now)\n",
    "        stations_data : DataFrame\n",
    "            Station information (id, lat, lon, capacity, etc.)\n",
    "        historical_data : DataFrame\n",
    "            Recent hourly aggregated data for computing lag features\n",
    "        current_inventory : dict\n",
    "            Current bike count at each station {station_id: bikes_count}\n",
    "        hours : int, optional\n",
    "            Prediction horizon (default: 8)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict with:\n",
    "            - predictions: Future demand predictions\n",
    "            - inventory: Projected inventory timeline\n",
    "            - needs: Rebalancing needs identified\n",
    "            - routes: Optimized truck routes\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# GENERATING REBALANCING PLAN\")\n",
    "        print(f\"# Current time: {current_time}\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        # Step 1: Predict demand for next N hours\n",
    "        predictions = self.predict_next_hours(\n",
    "            current_time, stations_data, historical_data, hours\n",
    "        )\n",
    "        \n",
    "        # Step 2: Simulate inventory\n",
    "        inventory_timeline = self.simulate_inventory(predictions, current_inventory)\n",
    "        \n",
    "        # Step 3: Identify rebalancing needs\n",
    "        rebalancing_needs = self.identify_rebalancing_needs(inventory_timeline)\n",
    "        \n",
    "        # Step 4: Optimize routes\n",
    "        if len(rebalancing_needs) > 0:\n",
    "            routes = self.optimize_routes(rebalancing_needs, stations_data)\n",
    "        else:\n",
    "            routes = pd.DataFrame()\n",
    "        \n",
    "        # Package results\n",
    "        plan = {\n",
    "            'predictions': predictions,\n",
    "            'inventory': inventory_timeline,\n",
    "            'needs': rebalancing_needs,\n",
    "            'routes': routes,\n",
    "            'generated_at': current_time\n",
    "        }\n",
    "        \n",
    "        self._print_summary(plan)\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def _print_summary(self, plan):\n",
    "        \"\"\"Print summary of rebalancing plan\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"REBALANCING PLAN SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Predictions summary\n",
    "        total_predicted_pickups = plan['predictions']['predicted_pickups'].sum()\n",
    "        total_predicted_dropoffs = plan['predictions']['predicted_dropoffs'].sum()\n",
    "        \n",
    "        print(f\"\\nDemand Forecast (next {self.prediction_horizon} hours):\")\n",
    "        print(f\"  Total pickups: {total_predicted_pickups:.0f}\")\n",
    "        print(f\"  Total dropoffs: {total_predicted_dropoffs:.0f}\")\n",
    "        print(f\"  Net system flow: {total_predicted_dropoffs - total_predicted_pickups:+.0f}\")\n",
    "        \n",
    "        # Inventory summary\n",
    "        stations_low = len(plan['inventory'][plan['inventory']['predicted_bikes'] < self.min_bikes_threshold])\n",
    "        stations_full = len(plan['inventory'][plan['inventory']['predicted_bikes'] > self.max_bikes_threshold])\n",
    "        \n",
    "        print(f\"\\nInventory Projections:\")\n",
    "        print(f\"  Stations going low (< {self.min_bikes_threshold} bikes): {stations_low}\")\n",
    "        print(f\"  Stations going full (> {self.max_bikes_threshold} bikes): {stations_full}\")\n",
    "        \n",
    "        # Rebalancing summary\n",
    "        if len(plan['needs']) > 0:\n",
    "            print(f\"\\nRebalancing Needs:\")\n",
    "            print(f\"  Stations requiring action: {len(plan['needs'])}\")\n",
    "            print(f\"  Total bikes to move: {plan['needs']['bikes_needed'].sum():.0f}\")\n",
    "            \n",
    "            if len(plan['routes']) > 0:\n",
    "                print(f\"\\nOptimized Routes:\")\n",
    "                print(f\"  Number of routes: {len(plan['routes'])}\")\n",
    "                print(f\"  Total distance: {plan['routes']['distance_km'].sum():.2f} km\")\n",
    "                print(f\"  Estimated time: {len(plan['routes']) * 20:.0f} minutes \" \n",
    "                      f\"(~20 min per route)\")\n",
    "        else:\n",
    "            print(f\"\\n‚úì NO REBALANCING NEEDED - All stations projected to stay within range\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def save_plan(self, plan, filename=None):\n",
    "        \"\"\"Save rebalancing plan to files\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = plan['generated_at'].strftime('%Y%m%d_%H%M')\n",
    "            filename = f'rebalancing_plan_{timestamp}'\n",
    "        \n",
    "        # Save predictions\n",
    "        plan['predictions'].to_csv(f'{filename}_predictions.csv', index=False)\n",
    "        \n",
    "        # Save inventory timeline\n",
    "        plan['inventory'].to_csv(f'{filename}_inventory.csv', index=False)\n",
    "        \n",
    "        # Save needs\n",
    "        if len(plan['needs']) > 0:\n",
    "            plan['needs'].to_csv(f'{filename}_needs.csv', index=False)\n",
    "        \n",
    "        # Save routes\n",
    "        if len(plan['routes']) > 0:\n",
    "            plan['routes'].to_csv(f'{filename}_routes.csv', index=False)\n",
    "        \n",
    "        print(f\"‚úì Rebalancing plan saved:\")\n",
    "        print(f\"  - {filename}_predictions.csv\")\n",
    "        print(f\"  - {filename}_inventory.csv\")\n",
    "        if len(plan['needs']) > 0:\n",
    "            print(f\"  - {filename}_needs.csv\")\n",
    "        if len(plan['routes']) > 0:\n",
    "            print(f\"  - {filename}_routes.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# SETUP: Load necessary data\n",
    "# ========================================================================\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = RebalancingOptimizer(\n",
    "    pickup_model_path='rf_model_pickups.joblib',\n",
    "    dropoff_model_path='rf_model_dropoffs.joblib',\n",
    "    station_capacity=20,\n",
    "    min_bikes_threshold=3,\n",
    "    max_bikes_threshold=17,\n",
    "    prediction_horizon=8\n",
    ")\n",
    "\n",
    "# ========================================================================\n",
    "# PREPARE DATA\n",
    "# ========================================================================\n",
    "\n",
    "# Example: Load station data\n",
    "# In production, this comes from your database\n",
    "stations_data = pd.DataFrame({\n",
    "    'station_id': [1, 27, 28, 39, 68, 182, 204, 426, 530],\n",
    "    'latitude': [42.3601, 42.3736, 42.3519, 42.3612, 42.3513, 42.3594, 42.3513, 42.3589, 42.3625],\n",
    "    'longitude': [-71.0589, -71.1097, -71.0629, -71.0812, -71.0644, -71.0637, -71.0956, -71.0707, -71.0598],\n",
    "    'member_ratio': [0.85, 0.78, 0.82, 0.75, 0.80, 0.88, 0.73, 0.81, 0.84],\n",
    "    'pickups_mean': [3.2, 2.8, 3.5, 2.1, 2.9, 4.2, 2.5, 3.1, 3.8],\n",
    "    'pickups_std': [4.5, 3.9, 4.8, 3.2, 4.1, 5.2, 3.7, 4.3, 4.9],\n",
    "    'pickups_max': [24, 18, 22, 15, 20, 28, 16, 21, 25],\n",
    "    'dropoffs_mean': [2.8, 2.5, 3.1, 1.9, 2.6, 3.8, 2.3, 2.8, 3.5],\n",
    "    'dropoffs_std': [4.1, 3.5, 4.5, 2.9, 3.8, 4.8, 3.4, 4.0, 4.6],\n",
    "    'dropoffs_max': [22, 16, 20, 14, 18, 25, 15, 19, 23]\n",
    "})\n",
    "\n",
    "# Example: Recent historical data (last 168 hours)\n",
    "# In production, this comes from your real-time data pipeline\n",
    "# For demo, create synthetic data\n",
    "historical_data = pd.DataFrame({\n",
    "    'station_id': np.repeat(stations_data['station_id'].values, 168),\n",
    "    'time': pd.date_range(\n",
    "        start=datetime.now() - timedelta(hours=168),\n",
    "        end=datetime.now(),\n",
    "        freq='1H'\n",
    "    ).repeat(len(stations_data)),\n",
    "    'pickups': np.random.poisson(lam=2, size=len(stations_data)*168),\n",
    "    'dropoffs': np.random.poisson(lam=2, size=len(stations_data)*168)\n",
    "})\n",
    "\n",
    "# Example: Current inventory (bikes at each station right now)\n",
    "# In production, this comes from real-time station sensors\n",
    "current_inventory = {\n",
    "    1: 5, 27: 12, 28: 8, 39: 15, 68: 3,\n",
    "    182: 18, 204: 7, 426: 2, 530: 19\n",
    "}\n",
    "\n",
    "# ========================================================================\n",
    "# GENERATE REBALANCING PLAN\n",
    "# ========================================================================\n",
    "\n",
    "current_time = datetime.now()\n",
    "\n",
    "plan = optimizer.generate_rebalancing_plan(\n",
    "    current_time=current_time,\n",
    "    stations_data=stations_data,\n",
    "    historical_data=historical_data,\n",
    "    current_inventory=current_inventory,\n",
    "    hours=8  # Predict next 8 hours\n",
    ")\n",
    "\n",
    "# ========================================================================\n",
    "# VIEW RESULTS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show predictions for next 4 hours\n",
    "print(\"\\nDemand Predictions (next 4 hours):\")\n",
    "print(plan['predictions'].head(4 * len(stations_data)))\n",
    "\n",
    "# Show critical rebalancing needs\n",
    "if len(plan['needs']) > 0:\n",
    "    print(\"\\nCritical Rebalancing Needs:\")\n",
    "    critical = plan['needs'][plan['needs']['priority'] == 'CRITICAL']\n",
    "    if len(critical) > 0:\n",
    "        print(critical[['station_id', 'action', 'bikes_needed', 'time', 'hours_until']])\n",
    "    else:\n",
    "        print(\"No CRITICAL needs\")\n",
    "    \n",
    "    # Show recommended routes\n",
    "    if len(plan['routes']) > 0:\n",
    "        print(\"\\nRecommended Routes (sorted by priority):\")\n",
    "        print(plan['routes'][['route_id', 'from_station', 'to_station', \n",
    "                                'bikes_to_move', 'distance_km', 'priority']].head(10))\n",
    "\n",
    "# ========================================================================\n",
    "# SAVE PLAN\n",
    "# ========================================================================\n",
    "\n",
    "optimizer.save_plan(plan)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REBALANCING PLAN COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review routes in the CSV files\")\n",
    "print(\"2. Dispatch trucks according to priority\")\n",
    "print(\"3. Update inventory as bikes are moved\")\n",
    "print(\"4. Monitor actual vs predicted demand\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0760f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87d4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ec93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1187d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
